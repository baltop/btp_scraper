#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê³ ë„í™”ëœ ì§€ì›ì‚¬ì—… ê³µê³  ìˆ˜ì§‘ í”„ë¡œê·¸ë¨
ì„¤ì • ê¸°ë°˜ ìŠ¤í¬ë˜í•‘ ì—”ì§„ì„ ì‚¬ìš©í•œ í™•ì¥ ê°€ëŠ¥í•œ ìŠ¤í¬ë˜í¼

ì£¼ìš” íŠ¹ì§•:
- ì„¤ì • íŒŒì¼ ê¸°ë°˜ ì‚¬ì´íŠ¸ ê´€ë¦¬ (sites_config.yaml)
- í”ŒëŸ¬ê·¸ì¸ ë°©ì‹ì˜ ìŠ¤í¬ë˜í¼ ì•„í‚¤í…ì²˜
- í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë¡œê¹…
- 100+ ì‚¬ì´íŠ¸ í™•ì¥ì„ ìœ„í•œ ìµœì í™”ëœ êµ¬ì¡°
"""

import argparse
import sys
import os
import logging
from pathlib import Path
from typing import List

from scraping_engine import ScrapingEngine, create_engine
from site_registry import get_registry

def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    
    # ë¡œê·¸ í¬ë§·
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setLevel(level)
    console_handler.setFormatter(formatter)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = logging.FileHandler('scraping.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

def validate_sites(engine: ScrapingEngine, sites: List[str]) -> List[str]:
    """ì‚¬ì´íŠ¸ ì½”ë“œ ìœ íš¨ì„± ê²€ì¦"""
    validation = engine.validate_sites(sites)
    
    if validation['invalid']:
        print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì‚¬ì´íŠ¸: {', '.join(validation['invalid'])}")
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸: {', '.join(engine.registry.get_site_codes())}")
        return validation['valid']
    
    return sites

def print_available_sites(engine: ScrapingEngine):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥"""
    registry = engine.registry
    
    print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ëª©ë¡:")
    print("=" * 60)
    
    # íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
    types = {}
    for site_code in registry.get_site_codes():
        config = registry.get_site_config(site_code)
        site_type = config.type
        if site_type not in types:
            types[site_type] = []
        types[site_type].append((site_code, config.name))
    
    for site_type, sites in types.items():
        type_desc = registry.scraper_types.get(site_type, {}).get('description', site_type)
        print(f"\nğŸ”§ {site_type.upper()} ({type_desc}):")
        for site_code, site_name in sorted(sites):
            print(f"  â€¢ {site_code:12} - {site_name}")
    
    print("\n" + "=" * 60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='ê³ ë„í™”ëœ ì§€ì›ì‚¬ì—… ê³µê³  ìˆ˜ì§‘ í”„ë¡œê·¸ë¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì œ:
  %(prog)s btp                           # ë¶€ì‚°í…Œí¬ë…¸íŒŒí¬ 4í˜ì´ì§€ ìˆ˜ì§‘
  %(prog)s btp itp ccei                  # ì—¬ëŸ¬ ì‚¬ì´íŠ¸ ë™ì‹œ ìˆ˜ì§‘  
  %(prog)s --all                         # ëª¨ë“  ì‚¬ì´íŠ¸ ìˆ˜ì§‘
  %(prog)s --type standard_table         # íŠ¹ì • íƒ€ì… ì‚¬ì´íŠ¸ë“¤ë§Œ ìˆ˜ì§‘
  %(prog)s btp --pages 2                 # 2í˜ì´ì§€ë§Œ ìˆ˜ì§‘
  %(prog)s --list                        # ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥
  %(prog)s --validate btp invalid_site   # ì‚¬ì´íŠ¸ ì½”ë“œ ìœ íš¨ì„± ê²€ì¦
        """
    )
    
    # ìœ„ì¹˜ ì¸ìˆ˜ - ì‚¬ì´íŠ¸ ì½”ë“œë“¤
    parser.add_argument(
        'sites',
        nargs='*',
        help='ìˆ˜ì§‘í•  ì‚¬ì´íŠ¸ ì½”ë“œ (ì˜ˆ: btp itp ccei)'
    )
    
    # ì„ íƒ ì¸ìˆ˜ë“¤
    parser.add_argument(
        '--all',
        action='store_true',
        help='ëª¨ë“  ë“±ë¡ëœ ì‚¬ì´íŠ¸ ìˆ˜ì§‘'
    )
    
    parser.add_argument(
        '--type',
        type=str,
        help='íŠ¹ì • íƒ€ì…ì˜ ì‚¬ì´íŠ¸ë“¤ë§Œ ìˆ˜ì§‘ (standard_table, ajax_api, javascript, session_based, playwright)'
    )
    
    parser.add_argument(
        '--pages',
        type=int,
        default=4,
        help='ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 4)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='output',
        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)'
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='sites_config.yaml',
        help='ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: sites_config.yaml)'
    )
    
    parser.add_argument(
        '--list',
        action='store_true',
        help='ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥'
    )
    
    parser.add_argument(
        '--validate',
        action='store_true',
        help='ì‚¬ì´íŠ¸ ì½”ë“œ ìœ íš¨ì„±ë§Œ ê²€ì¦'
    )
    
    parser.add_argument(
        '--continue-on-error',
        action='store_true',
        default=True,
        help='ì˜¤ë¥˜ ë°œìƒ ì‹œ ê³„ì† ì§„í–‰ (ê¸°ë³¸ê°’: True)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥'
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)
    
    try:
        # ìŠ¤í¬ë˜í•‘ ì—”ì§„ ìƒì„±
        engine = create_engine(args.config)
        engine.set_continue_on_error(args.continue_on_error)
        
        logger.info(f"ìŠ¤í¬ë˜í•‘ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {len(engine.registry.get_site_codes())}ê°œ ì‚¬ì´íŠ¸ ë“±ë¡")
        
        # ì‚¬ì´íŠ¸ ëª©ë¡ ì¶œë ¥
        if args.list:
            print_available_sites(engine)
            return 0
        
        # ìˆ˜ì§‘í•  ì‚¬ì´íŠ¸ ê²°ì •
        sites_to_scrape = []
        
        if args.all:
            sites_to_scrape = engine.registry.get_site_codes()
            logger.info("ëª¨ë“  ì‚¬ì´íŠ¸ ìˆ˜ì§‘ ëª¨ë“œ")
        elif args.type:
            sites_to_scrape = engine.registry.get_sites_by_type(args.type)
            if not sites_to_scrape:
                print(f"âŒ íƒ€ì… '{args.type}'ì— í•´ë‹¹í•˜ëŠ” ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return 1
            logger.info(f"íƒ€ì…ë³„ ìˆ˜ì§‘ ëª¨ë“œ: {args.type}")
        elif args.sites:
            sites_to_scrape = args.sites
            logger.info(f"ì§€ì •ëœ ì‚¬ì´íŠ¸ ìˆ˜ì§‘: {', '.join(sites_to_scrape)}")
        else:
            print("âŒ ìˆ˜ì§‘í•  ì‚¬ì´íŠ¸ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”")
            print("ì‚¬ìš©ë²•: %(prog)s <ì‚¬ì´íŠ¸ì½”ë“œ1> [ì‚¬ì´íŠ¸ì½”ë“œ2] ... ë˜ëŠ” --all ë˜ëŠ” --type <íƒ€ì…>" % {'prog': sys.argv[0]})
            print("ìì„¸í•œ ë„ì›€ë§: %(prog)s --help" % {'prog': sys.argv[0]})
            return 1
        
        # ì‚¬ì´íŠ¸ ìœ íš¨ì„± ê²€ì¦
        if args.validate:
            valid_sites = validate_sites(engine, sites_to_scrape)
            print(f"âœ… ìœ íš¨í•œ ì‚¬ì´íŠ¸: {', '.join(valid_sites)}")
            return 0
        
        sites_to_scrape = validate_sites(engine, sites_to_scrape)
        if not sites_to_scrape:
            print("âŒ ìœ íš¨í•œ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 1
        
        # í˜ì´ì§€ ìˆ˜ ê²€ì¦
        if args.pages < 1:
            print("âŒ í˜ì´ì§€ ìˆ˜ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤")
            return 1
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        print(f"\nğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        print(f"ğŸ“ ëŒ€ìƒ ì‚¬ì´íŠ¸: {', '.join(sites_to_scrape)}")
        print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜: {args.pages}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output}")
        print(f"âš™ï¸  ì„¤ì • íŒŒì¼: {args.config}")
        print()
        
        # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        if len(sites_to_scrape) == 1:
            # ë‹¨ì¼ ì‚¬ì´íŠ¸
            result = engine.scrape_site(
                sites_to_scrape[0], 
                max_pages=args.pages, 
                output_dir=os.path.join(args.output, sites_to_scrape[0])
            )
        else:
            # ë‹¤ì¤‘ ì‚¬ì´íŠ¸
            results = engine.scrape_sites(
                sites_to_scrape,
                max_pages=args.pages,
                output_base=args.output
            )
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        engine.print_summary()
        
        # ì„±ê³µí•œ ì‚¬ì´íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        results = engine.get_results()
        successful = sum(1 for r in results.values() if r['status'] == 'completed')
        
        if successful > 0:
            print(f"\nâœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {successful}/{len(sites_to_scrape)} ì‚¬ì´íŠ¸ ì„±ê³µ")
            return 0
        else:
            print(f"\nâŒ ëª¨ë“  ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            return 1
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        return 130
    except FileNotFoundError as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        return 2
    except Exception as e:
        logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())