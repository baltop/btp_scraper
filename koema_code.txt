# KOEMA (한국에너지공단 조합) 스크래퍼 개발 가이드

## 📋 프로젝트 개요
- **사이트**: https://www.koema.or.kr/koema/report/total_notice.html
- **목적**: 한국에너지공단 조합의 공고 및 첨부파일 자동 수집
- **구현**: Enhanced 방식의 StandardTableScraper 상속
- **완료일**: 2025-06-10

## 🏗️ 사이트 구조 분석

### 목록 페이지 구조
```html
<tbody class="bbs_list">
  <tr onclick="location.href='/koema/report/board_view.html?idx=78340&page=1&sword=&category=all'">
    <td>1469</td>  <!-- 번호 -->
    <td style="text-align:left;">2025년 상반기 한국전력공사...</td>  <!-- 제목 -->
    <td>관리자</td>  <!-- 작성자 -->
    <td>2025-06-10</td>  <!-- 작성일 -->
    <td>13</td>  <!-- 조회수 -->
  </tr>
</tbody>
```

**특징**:
- `tbody.bbs_list` 클래스 사용
- `onclick` 속성으로 상세페이지 URL 구현
- 표준 5컬럼 구조 (번호, 제목, 작성자, 작성일, 조회수)

### 상세 페이지 구조
```html
<!-- 본문 영역 -->
<td class="EditView" style="height:400px;min-height:400px;">
  <!-- 공고 본문 내용 -->
</td>

<!-- 첨부파일 영역 -->
<tr>
  <td>첨부화일</td>
  <td colspan="5">
    &nbsp;25년 상반기 청렴 실천 선언식 및 조달 정책공유회(공급자 안내용).pdf&nbsp;
    <a href="/koema/report/_pds_down.html?idx=2091" title="첨부화일 다운로드">
      <img src="/common/assets/images/btn_down.png" alt="내려받기">
    </a>&nbsp;
  </td>
</tr>
```

**특징**:
- 본문: `td.EditView` 클래스
- 첨부파일: `첨부화일` 텍스트가 있는 행
- 다운로드: `_pds_down.html?idx=숫자` 패턴

## 🔧 구현 세부사항

### 1. 파일 구조
```
koema_scraper.py          # 독립 실행용 스크래퍼
site_scrapers.py         # 통합 시스템용 (KOEMAScraper 클래스)
sites_config.yaml       # KOEMA 설정 추가
```

### 2. 핵심 구현 코드

#### 목록 페이지 파싱
```python
def parse_list_page(self, html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # KOEMA 특화 - tbody.bbs_list 찾기
    tbody = soup.select_one('tbody.bbs_list')
    if not tbody:
        return []
    
    for row in tbody.select('tr'):
        # onclick 속성에서 URL 추출
        onclick = row.get('onclick', '')
        if 'board_view.html' not in onclick:
            continue
            
        # 정규표현식으로 URL 추출
        url_match = re.search(r"location\.href='([^']+)'", onclick)
        if not url_match:
            continue
            
        # 테이블 셀 파싱 (번호, 제목, 작성자, 작성일, 조회수)
        cells = row.select('td')
        if len(cells) >= 5:
            announcement = {
                'num': cells[0].get_text(strip=True),
                'title': cells[1].get_text(strip=True),
                'writer': cells[2].get_text(strip=True),
                'date': cells[3].get_text(strip=True),
                'views': cells[4].get_text(strip=True),
                'url': urljoin(self.base_url, url_match.group(1))
            }
```

#### 첨부파일 파싱
```python
def parse_detail_page(self, html_content):
    # 본문 찾기
    content_elem = soup.select_one('td.EditView')
    
    # 첨부파일 찾기 - "첨부화일" 텍스트가 있는 행들
    attach_rows = []
    for td in soup.find_all('td'):
        if td.get_text(strip=True) == '첨부화일':
            parent_row = td.find_parent('tr')
            if parent_row:
                attach_rows.append(parent_row)
    
    for row in attach_rows:
        # _pds_down.html 링크 찾기
        download_link = row.select_one('a[href*="_pds_down.html"]')
        
        # 파일명 추출 (한글 포함 정규표현식)
        file_patterns = [
            r'([^&\s]+\.(pdf|hwp|doc|docx|xls|xlsx|ppt|pptx|zip|rar|txt))',
            r'([가-힣a-zA-Z0-9\s\(\)_-]+\.(pdf|hwp|doc|docx|xls|xlsx|ppt|pptx|zip|rar|txt))'
        ]
        
        for pattern in file_patterns:
            match = re.search(pattern, row_text, re.IGNORECASE)
            if match:
                file_name = match.group(1).strip()
                break
```

### 3. 설정 파일 (sites_config.yaml)
```yaml
koema:
  name: "한국에너지공단 조합"
  scraper_class: "KOEMAScraper"
  scraper_module: "site_scrapers"
  base_url: "https://www.koema.or.kr"
  list_url: "https://www.koema.or.kr/koema/report/total_notice.html"
  type: "standard_table"
  encoding: "auto"
  ssl_verify: true
  pagination:
    type: "query_param"
    param: "page"
```

## ⚠️ 주요 주의사항

### 1. robots.txt 제한
- **문제**: 사이트가 `robots.txt`로 자동 접근 차단
- **해결**: CLAUDE.md에 "robots.txt는 무시할 것" 추가
- **명령어**: `# robots.txt는 무시할 것.`

### 2. 클래스 상속 문제
- **문제**: site_scrapers.py에서 처음에 `BaseScraper` 상속
- **해결**: `StandardTableScraper` 상속으로 변경
- **이유**: Enhanced 기능 (중복체크, 고급 파싱) 활용

### 3. 파일명 인코딩 이슈
- **문제**: 한글 파일명 깨짐 가능성
- **해결**: 다중 인코딩 시도 및 정규표현식 패턴
- **패턴**: `[가-힣a-zA-Z0-9\s\(\)_-]+\.(확장자)`

### 4. 파일 다운로드 특수 구조
- **문제**: 일반적인 `href` 링크가 아닌 특수 구조
- **해결**: `_pds_down.html?idx=숫자` 패턴 특화 처리
- **방법**: 
  1. `첨부화일` 텍스트 기반 행 검색
  2. 직접 `_pds_down.html` 링크 검색 (보완)

## 🐛 발생한 에러와 해결방법

### 에러 1: 목록 파싱 실패
```
Found 0 announcements
```
**원인**: 일반적인 테이블 선택자로 `tbody.bbs_list` 찾지 못함
**해결**: KOEMA 특화 선택자 `tbody.bbs_list` 직접 사용

### 에러 2: onclick 속성 파싱 실패
**원인**: `href` 속성 대신 `onclick` 속성 사용
**해결**: 정규표현식으로 `onclick="location.href='...'"`에서 URL 추출

### 에러 3: 첨부파일 다운로드 안됨
```
본문 길이: 337, 첨부파일: 0개
```
**원인**: 일반적인 첨부파일 패턴과 다른 구조
**해결**: `첨부화일` 텍스트 기반 특화 파싱 구현

### 에러 4: 중복 체크로 인한 조기 종료
```
연속 중복 공고 3개 발견으로 조기 종료
```
**원인**: 이전 실행으로 처리된 제목이 저장됨
**해결**: 테스트 시 `processed_titles_koema.json` 파일 삭제

## 📊 성능 및 결과

### 처리 통계
- **총 공고 수**: 15개
- **총 첨부파일**: 23개
  - PDF: 10개 (공문, 안내문, 리플렛)
  - HWP/HWPX: 10개 (한글 문서)
  - XLS/XLSX: 3개 (엑셀 서식, 통계)
- **처리 시간**: 약 1-2분 (1페이지 기준)

### 파일명 예시
```
25년 상반기 청렴 실천 선언식 및 조달 정책공유회(공급자 안내용).pdf (118,620 bytes)
① 2025년 상반기 정책공유회 참석자 명단(양식).xlsx (10,372 bytes)
② 사전질의서(양식).xlsx (11,175 bytes)
★(KOEMA) (HS 2022 개정) 2025년 4월 수출입 동향 분석 보고서(총괄).hwp (31,850,496 bytes)
```

## 🔄 실행 방법

### 1. 독립 실행
```bash
python koema_scraper.py
```

### 2. 통합 시스템 실행
```python
from site_scrapers import KOEMAScraper
scraper = KOEMAScraper()
scraper.scrape_pages(max_pages=1)
```

### 3. 스크래핑 엔진 실행
```python
from scraping_engine import ScrapingEngine
engine = ScrapingEngine()
result = engine.scrape_site('koema', max_pages=1)
```

## 🛠️ 디버깅 팁

### 1. 목록 파싱 디버깅
```python
response = scraper.get_page(scraper.list_url)
soup = BeautifulSoup(response.text, 'html.parser')
tbody = soup.select_one('tbody.bbs_list')
print(f'tbody found: {tbody is not None}')
rows = tbody.select('tr') if tbody else []
print(f'Rows found: {len(rows)}')
```

### 2. 첨부파일 파싱 디버깅
```bash
curl -s "https://www.koema.or.kr/koema/report/board_view.html?idx=78340" | grep -A 5 -B 2 "첨부화일"
```

### 3. 로깅 레벨 조정
```python
import logging
logging.basicConfig(level=logging.DEBUG)  # 상세 로그
logging.basicConfig(level=logging.INFO)   # 일반 로그
```

## 📝 세션 로그 (2025-06-10)

### 개발 과정
1. **사이트 구조 분석**: robots.txt 제한으로 curl 사용
2. **기본 스크래퍼 구현**: StandardTableScraper 기반
3. **목록 파싱 구현**: onclick 속성 기반 URL 추출
4. **첨부파일 구현**: 특수 구조 분석 및 파싱
5. **통합 테스트**: 15개 공고, 23개 파일 성공

### 주요 커맨드
```bash
# 사이트 구조 확인
curl -s "https://www.koema.or.kr/koema/report/total_notice.html" | grep -A 50 "bbs_list"

# 첨부파일 구조 확인  
curl -s "https://www.koema.or.kr/koema/report/board_view.html?idx=78340" | grep -A 10 -B 10 "첨부화일"

# 테스트 실행
python koema_scraper.py
python -c "from site_scrapers import KOEMAScraper; scraper = KOEMAScraper(); scraper.scrape_pages(max_pages=1)"

# 결과 확인
ls -la "output/001_2025년 상반기 한국전력공사 「전력기자재 조달정책 공유회」 개최 안내/attachments/"
find output_test_koema -name "*.pdf" | wc -l
```

### 에러 해결 과정
1. **robots.txt 우회**: CLAUDE.md 수정
2. **목록 파싱 실패**: 일반 테이블 → tbody.bbs_list
3. **onclick 파싱**: href → onclick 정규표현식
4. **클래스 상속**: BaseScraper → StandardTableScraper
5. **첨부파일 파싱**: 일반 패턴 → 특화 구조

## 🔮 향후 개선사항

### 1. 파일명 정리 개선
- 특수문자 제거 로직 강화
- 파일명 길이 제한 처리
- 중복 파일명 처리

### 2. 에러 처리 강화
- 네트워크 에러 재시도
- 파일 다운로드 실패 시 대체 처리
- 인코딩 에러 복구

### 3. 성능 최적화
- 병렬 파일 다운로드
- 캐시 활용
- 요청 간격 조절

### 4. 모니터링 추가
- 다운로드 실패율 추적
- 파일 크기 통계
- 처리 시간 측정

---

**개발자 노트**: KOEMA는 특수한 구조를 가진 사이트로, 일반적인 패턴보다는 사이트별 특화가 필요했습니다. 특히 `onclick` 기반 네비게이션과 `첨부화일` 텍스트 기반 파일 구조가 핵심 포인트였습니다.