#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced ìŠ¤í¬ë˜í¼ í†µí•© ì‹¤í–‰ê¸°
ëª¨ë“  Enhanced ìŠ¤í¬ë˜í¼ë“¤ì„ 3ê°œì”© ë™ì‹œì— ì‹¤í–‰
"""

import os
import sys
import logging
import asyncio
import concurrent.futures
from datetime import datetime
import time
from typing import List, Dict, Any

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Enhanced ìŠ¤í¬ë˜í¼ë“¤ import
from enhanced_btp_scraper import EnhancedBTPScraper
from enhanced_cci_scraper import EnhancedCCIScraper
from enhanced_ccei_scraper import EnhancedCCEIScraper
from enhanced_cepa_scraper import EnhancedCEPAScraper
from enhanced_dcb_scraper import EnhancedDCBScraper
from enhanced_djbea_scraper import EnhancedDJBEAScraper
from enhanced_gib_scraper import EnhancedGIBScraper
from enhanced_gsif_scraper import EnhancedGSIFScraper
from enhanced_itp_scraper import EnhancedITPScraper
from enhanced_jbf_scraper import EnhancedJBFScraper
from enhanced_kdata_scraper import EnhancedKdataScraper
from enhanced_kidp_scraper import EnhancedKIDPScraper
from enhanced_koema_scraper import EnhancedKOEMAScraper
from enhanced_mire_scraper import EnhancedMIREScraper
from enhanced_keit_scraper import EnhancedKEITScraper
from enhanced_kca_scraper import EnhancedKCAScraper
from enhanced_smtech_scraper import EnhancedSMTECHScraper
from enhanced_jepa_scraper import EnhancedJEPAScraper
from enhanced_kmedihub_scraper import EnhancedKMEDIHUBScraper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_scrapers_execution.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Enhanced ìŠ¤í¬ë˜í¼ ì •ì˜
ENHANCED_SCRAPERS = {
    'btp': {
        'class': EnhancedBTPScraper,
        'name': 'BTP (ë¶€ì‚°í…Œí¬ë…¸íŒŒí¬)',
        'output_dir': 'btp_enhanced'
    },
    'cci': {
        'class': EnhancedCCIScraper,
        'name': 'CCI (ì°½ì¡°ê²½ì œí˜ì‹ ì„¼í„°)',
        'output_dir': 'cci_enhanced'
    },
    'ccei': {
        'class': EnhancedCCEIScraper,
        'name': 'CCEI (ì°½ì¡°ê²½ì œì—°êµ¬ì›)',
        'output_dir': 'ccei_enhanced'
    },
    'cepa': {
        'class': EnhancedCEPAScraper,
        'name': 'CEPA (ì¤‘ì•™í™˜ê²½ì‚°ì—…ì—°êµ¬ì›)',
        'output_dir': 'cepa_enhanced'
    },
    'dcb': {
        'class': EnhancedDCBScraper,
        'name': 'DCB (ëŒ€êµ¬ë””ì§€í„¸ì‚°ì—…ì§„í¥ì›)',
        'output_dir': 'dcb_enhanced'
    },
    'djbea': {
        'class': EnhancedDJBEAScraper,
        'name': 'DJBEA (ëŒ€ì „ë°”ì´ì˜¤ì§„í¥ì›)',
        'output_dir': 'djbea_enhanced'
    },
    'gib': {
        'class': EnhancedGIBScraper,
        'name': 'GIB (ê²½ê¸°ë°”ì´ì˜¤ì„¼í„°)',
        'output_dir': 'gib_enhanced'
    },
    'gsif': {
        'class': EnhancedGSIFScraper,
        'name': 'GSIF (ê°•ë¦‰ê³¼í•™ì‚°ì—…ì§„í¥ì›)',
        'output_dir': 'gsif_enhanced'
    },
    'itp': {
        'class': EnhancedITPScraper,
        'name': 'ITP (ì¸ì²œí…Œí¬ë…¸íŒŒí¬)',
        'output_dir': 'itp_enhanced'
    },
    'jbf': {
        'class': EnhancedJBFScraper,
        'name': 'JBF (ì „ë‚¨ë°”ì´ì˜¤ì§„í¥ì›)',
        'output_dir': 'jbf_enhanced'
    },
    'kdata': {
        'class': EnhancedKdataScraper,
        'name': 'KDATA (í•œêµ­ë°ì´í„°ì‚°ì—…ì§„í¥ì›)',
        'output_dir': 'kdata_enhanced'
    },
    'kidp': {
        'class': EnhancedKIDPScraper,
        'name': 'KIDP (í•œêµ­ë””ìì¸ì§„í¥ì›)',
        'output_dir': 'kidp_enhanced'
    },
    'koema': {
        'class': EnhancedKOEMAScraper,
        'name': 'KOEMA (í•œêµ­ì—ë„ˆì§€ê³µë‹¨)',
        'output_dir': 'koema_enhanced'
    },
    'mire': {
        'class': EnhancedMIREScraper,
        'name': 'MIRE (í•´ì–‘ìˆ˜ì‚°ê³¼í•™ê¸°ìˆ ì§„í¥ì›)',
        'output_dir': 'mire_enhanced'
    },
    'keit': {
        'class': EnhancedKEITScraper,
        'name': 'KEIT (í•œêµ­ì‚°ì—…ê¸°ìˆ ê¸°íší‰ê°€ì›)',
        'output_dir': 'keit_enhanced'
    },
    'kca': {
        'class': EnhancedKCAScraper,
        'name': 'KCA (í•œêµ­ë°©ì†¡í†µì‹ ì „íŒŒì§„í¥ì›)',
        'output_dir': 'kca_enhanced'
    },
    'smtech': {
        'class': EnhancedSMTECHScraper,
        'name': 'SMTECH (ì¤‘ì†Œê¸°ì—…ê¸°ìˆ ì •ë³´ì§„í¥ì›)',
        'output_dir': 'smtech_enhanced'
    },
    'jepa': {
        'class': EnhancedJEPAScraper,
        'name': 'JEPA (ì¤‘ì†Œê¸°ì—…ì¼ìë¦¬ê²½ì œì§„í¥ì›)',
        'output_dir': 'jepa_enhanced'
    },
    'kmedihub': {
        'class': EnhancedKMEDIHUBScraper,
        'name': 'KMEDIHUB (í•œêµ­ì˜ë£Œê¸°ê¸°ì•ˆì „ì •ë³´ì›)',
        'output_dir': 'kmedihub_enhanced'
    }
}

def run_single_scraper(scraper_config: Dict[str, Any], max_pages: int = 3) -> Dict[str, Any]:
    """ë‹¨ì¼ ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
    scraper_key = scraper_config['key']
    scraper_info = scraper_config['info']
    
    start_time = time.time()
    
    try:
        logger.info(f"ğŸš€ [{scraper_key.upper()}] {scraper_info['name']} ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        # ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        scraper_class = scraper_info['class']
        scraper = scraper_class()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = f"./output/{scraper_info['output_dir']}"
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        scraper.scrape_pages(max_pages=max_pages, output_base=output_dir)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ê²°ê³¼ í†µê³„ ìˆ˜ì§‘
        stats = collect_scraper_stats(output_dir)
        stats.update({
            'scraper': scraper_key,
            'name': scraper_info['name'],
            'status': 'success',
            'duration': duration,
            'output_dir': output_dir
        })
        
        logger.info(f"âœ… [{scraper_key.upper()}] ì™„ë£Œ - {duration:.1f}ì´ˆ, ê³µê³  {stats['announcements']}ê°œ, íŒŒì¼ {stats['files']}ê°œ")
        
        return stats
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        logger.error(f"âŒ [{scraper_key.upper()}] ì‹¤íŒ¨ - {e}")
        
        return {
            'scraper': scraper_key,
            'name': scraper_info['name'],
            'status': 'error',
            'error': str(e),
            'duration': duration,
            'announcements': 0,
            'files': 0,
            'total_size': 0
        }

def collect_scraper_stats(output_dir: str) -> Dict[str, Any]:
    """ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ê²°ê³¼ í†µê³„ ìˆ˜ì§‘"""
    stats = {
        'announcements': 0,
        'files': 0,
        'total_size': 0
    }
    
    try:
        if not os.path.exists(output_dir):
            return stats
        
        # ê³µê³  í´ë”ë“¤ ì°¾ê¸°
        announcement_folders = [
            item for item in os.listdir(output_dir)
            if os.path.isdir(os.path.join(output_dir, item)) and 
               any(item.startswith(prefix) for prefix in ['001_', '002_', '003_', '004_', '005_', '006_', '007_', '008_', '009_'])
        ]
        
        stats['announcements'] = len(announcement_folders)
        
        # ì²¨ë¶€íŒŒì¼ í†µê³„
        for folder in announcement_folders:
            attachments_dir = os.path.join(output_dir, folder, 'attachments')
            if os.path.exists(attachments_dir):
                for file in os.listdir(attachments_dir):
                    file_path = os.path.join(attachments_dir, file)
                    if os.path.isfile(file_path):
                        stats['files'] += 1
                        stats['total_size'] += os.path.getsize(file_path)
        
    except Exception as e:
        logger.error(f"í†µê³„ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return stats

def run_scrapers_batch(scraper_batch: List[Dict[str, Any]], max_pages: int = 3) -> List[Dict[str, Any]]:
    """ìŠ¤í¬ë˜í¼ ë°°ì¹˜ ì‹¤í–‰ (ìµœëŒ€ 3ê°œ ë™ì‹œ)"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        # ê° ìŠ¤í¬ë˜í¼ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        future_to_scraper = {
            executor.submit(run_single_scraper, config, max_pages): config['key']
            for config in scraper_batch
        }
        
        results = []
        
        # ì™„ë£Œëœ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
        for future in concurrent.futures.as_completed(future_to_scraper):
            scraper_key = future_to_scraper[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"ìŠ¤í¬ë˜í¼ {scraper_key} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                results.append({
                    'scraper': scraper_key,
                    'status': 'exception',
                    'error': str(e),
                    'duration': 0,
                    'announcements': 0,
                    'files': 0,
                    'total_size': 0
                })
        
        return results

def format_size(size_bytes: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f} {size_names[i]}"

def print_summary(all_results: List[Dict[str, Any]]):
    """ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ¯ Enhanced ìŠ¤í¬ë˜í¼ ì „ì²´ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    successful = [r for r in all_results if r['status'] == 'success']
    failed = [r for r in all_results if r['status'] in ['error', 'exception']]
    
    total_duration = sum(r['duration'] for r in all_results)
    total_announcements = sum(r['announcements'] for r in successful)
    total_files = sum(r['files'] for r in successful)
    total_size = sum(r['total_size'] for r in successful)
    
    print(f"ğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  â€¢ ì´ ìŠ¤í¬ë˜í¼: {len(all_results)}ê°œ")
    print(f"  â€¢ ì„±ê³µ: {len(successful)}ê°œ")
    print(f"  â€¢ ì‹¤íŒ¨: {len(failed)}ê°œ")
    print(f"  â€¢ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
    print(f"  â€¢ ì´ ìˆ˜ì§‘ ê³µê³ : {total_announcements}ê°œ")
    print(f"  â€¢ ì´ ë‹¤ìš´ë¡œë“œ íŒŒì¼: {total_files}ê°œ")
    print(f"  â€¢ ì´ íŒŒì¼ í¬ê¸°: {format_size(total_size)}")
    
    if successful:
        print(f"\nâœ… ì„±ê³µí•œ ìŠ¤í¬ë˜í¼ë“¤:")
        for result in successful:
            print(f"  â€¢ {result['name']}: {result['announcements']}ê°œ ê³µê³ , {result['files']}ê°œ íŒŒì¼, {result['duration']:.1f}ì´ˆ")
    
    if failed:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼ë“¤:")
        for result in failed:
            print(f"  â€¢ {result['name']}: {result.get('error', 'Unknown error')}")
    
    print("\n" + "="*80)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Enhanced ìŠ¤í¬ë˜í¼ í†µí•© ì‹¤í–‰ê¸° ì‹œì‘")
    print("="*60)
    
    start_time = datetime.now()
    logger.info("Enhanced ìŠ¤í¬ë˜í¼ í†µí•© ì‹¤í–‰ ì‹œì‘")
    
    # ìŠ¤í¬ë˜í¼ ì„¤ì • ì¤€ë¹„
    scraper_configs = []
    for key, info in ENHANCED_SCRAPERS.items():
        scraper_configs.append({
            'key': key,
            'info': info
        })
    
    print(f"ğŸ“‹ ì´ {len(scraper_configs)}ê°œ Enhanced ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì˜ˆì •")
    print("   3ê°œì”© ë™ì‹œ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰")
    print()
    
    # 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batch_size = 3
    all_results = []
    
    for i in range(0, len(scraper_configs), batch_size):
        batch = scraper_configs[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        
        print(f"ğŸ”„ ë°°ì¹˜ {batch_num} ì‹¤í–‰ ì¤‘ ({len(batch)}ê°œ ìŠ¤í¬ë˜í¼):")
        for config in batch:
            print(f"   â€¢ {config['info']['name']}")
        print()
        
        # ë°°ì¹˜ ì‹¤í–‰
        batch_results = run_scrapers_batch(batch, max_pages=10)
        all_results.extend(batch_results)
        
        print(f"âœ¨ ë°°ì¹˜ {batch_num} ì™„ë£Œ\n")
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    logger.info(f"Enhanced ìŠ¤í¬ë˜í¼ í†µí•© ì‹¤í–‰ ì™„ë£Œ - ì´ {total_duration:.1f}ì´ˆ")
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_summary(all_results)
    
    print(f"\nâ° ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
    print(f"ğŸ“… ì‹¤í–‰ ì™„ë£Œ ì‹œê°: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    return all_results

if __name__ == "__main__":
    try:
        results = main()
        print("\nğŸ‰ ëª¨ë“  Enhanced ìŠ¤í¬ë˜í¼ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì‹¤í–‰ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)