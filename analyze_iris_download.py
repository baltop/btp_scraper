#!/usr/bin/env python3
"""
IRIS ì‚¬ì´íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¸Œë¼ìš°ì € í™˜ê²½ì—ì„œ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§
"""

import asyncio
import json
import os
from playwright.async_api import async_playwright
from urllib.parse import urlparse, parse_qs
import time


class IrisDownloadAnalyzer:
    def __init__(self):
        self.base_url = "https://www.iris.go.kr"
        self.list_url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituListView.do"
        self.network_requests = []
        self.network_responses = []
        self.cookies = []
        
    async def analyze_download_mechanism(self):
        """IRIS ì‚¬ì´íŠ¸ì˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„"""
        
        async with async_playwright() as p:
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¥¼ Falseë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì œ ë¸Œë¼ìš°ì € í™˜ê²½ ì¬í˜„
            browser = await p.chromium.launch(
                headless=False,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            try:
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                )
                
                page = await context.new_page()
                
                # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§ ì„¤ì •
                await self._setup_network_monitoring(page)
                
                print("ğŸ” IRIS ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
                await page.goto(self.list_url, wait_until='networkidle')
                
                # ì´ˆê¸° ì¿ í‚¤ ìˆ˜ì§‘
                cookies = await context.cookies()
                self.cookies = cookies
                print(f"ğŸ“‹ ì´ˆê¸° ì¿ í‚¤ ìˆ˜ì§‘: {len(cookies)}ê°œ")
                
                # ì²« ë²ˆì§¸ ê³µê³  ì°¾ê¸° ë° í´ë¦­
                print("ğŸ” ì²« ë²ˆì§¸ ê³µê³  ì°¾ëŠ” ì¤‘...")
                await self._find_and_click_first_announcement(page)
                
                # ìƒì„¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                await page.wait_for_load_state('networkidle')
                
                # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸° ë° ë¶„ì„
                print("ğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ë¶„ì„ ì¤‘...")
                await self._analyze_download_links(page)
                
                # ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
                print("â¬‡ï¸ ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘...")
                await self._attempt_file_download(page)
                
                # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
                await self._analyze_results()
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
            finally:
                print("ğŸ” ë¶„ì„ ì™„ë£Œ. ë¸Œë¼ìš°ì €ë¥¼ 5ì´ˆ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                await asyncio.sleep(5)
                await browser.close()
    
    async def _setup_network_monitoring(self, page):
        """ë„¤íŠ¸ì›Œí¬ ìš”ì²­/ì‘ë‹µ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        
        async def handle_request(request):
            self.network_requests.append({
                'url': request.url,
                'method': request.method,
                'headers': dict(request.headers),
                'post_data': request.post_data,
                'timestamp': time.time()
            })
            
        async def handle_response(response):
            self.network_responses.append({
                'url': response.url,
                'status': response.status,
                'headers': dict(response.headers),
                'timestamp': time.time()
            })
            
        page.on('request', handle_request)
        page.on('response', handle_response)
    
    async def _find_and_click_first_announcement(self, page):
        """ì²« ë²ˆì§¸ ê³µê³  ì°¾ê¸° ë° í´ë¦­"""
        try:
            # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ê³µê³  ë§í¬ ì°¾ê¸°
            selectors = [
                'table.table-list tbody tr:first-child td a',
                'table tbody tr:first-child a',
                '.list-table tbody tr:first-child a',
                'tbody tr:first-child a[href*="retrieveBsnsAncmBtinSituDetailView"]'
            ]
            
            first_link = None
            for selector in selectors:
                first_link = await page.query_selector(selector)
                if first_link:
                    print(f"âœ… ê³µê³  ë§í¬ ë°œê²¬: {selector}")
                    break
            
            if not first_link:
                print("âŒ ì²« ë²ˆì§¸ ê³µê³  ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ì¶œë ¥
                all_links = await page.query_selector_all('a')
                print(f"ğŸ“‹ í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ëª¨ë“  ë§í¬: {len(all_links)}ê°œ")
                for i, link in enumerate(all_links[:10]):  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                    href = await link.get_attribute('href')
                    text = await link.text_content()
                    print(f"  {i+1}. {text[:50]} -> {href}")
                return
            
            # ë§í¬ í´ë¦­
            await first_link.click()
            print("âœ… ì²« ë²ˆì§¸ ê³µê³  í´ë¦­ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ê³µê³  í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _analyze_download_links(self, page):
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ë¶„ì„"""
        try:
            # JavaScriptì—ì„œ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì°¾ê¸°
            download_links = await page.query_selector_all('a[onclick*="downloadAtchFile"], a[href*="download"]')
            
            if not download_links:
                print("âŒ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                # í˜ì´ì§€ ë‚´ìš© í™•ì¸
                content = await page.content()
                if 'downloadAtchFile' in content:
                    print("âœ… downloadAtchFile í•¨ìˆ˜ ë°œê²¬")
                    # í•¨ìˆ˜ ì •ì˜ ì¶”ì¶œ
                    await page.evaluate("""
                        if (window.f_bsnsAncm_downloadAtchFile) {
                            console.log('f_bsnsAncm_downloadAtchFile í•¨ìˆ˜:', window.f_bsnsAncm_downloadAtchFile.toString());
                        }
                    """)
                return
            
            print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ë§í¬ {len(download_links)}ê°œ ë°œê²¬")
            
            for i, link in enumerate(download_links):
                onclick = await link.get_attribute('onclick')
                href = await link.get_attribute('href')
                text = await link.text_content()
                
                print(f"  {i+1}. {text}")
                print(f"     onclick: {onclick}")
                print(f"     href: {href}")
                
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ë§í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _attempt_file_download(self, page):
        """ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„"""
        try:
            # ë‹¤ìš´ë¡œë“œ ë§í¬ í´ë¦­ ì „ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì¹´ìš´íŠ¸
            before_count = len(self.network_requests)
            
            # ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸° ë° í´ë¦­
            download_link = await page.query_selector('a[onclick*="downloadAtchFile"]')
            if not download_link:
                download_link = await page.query_selector('a[href*="download"]')
            
            if download_link:
                print("ğŸ“¥ ë‹¤ìš´ë¡œë“œ ë§í¬ í´ë¦­...")
                
                # ë‹¤ìš´ë¡œë“œ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§
                async def handle_download(download):
                    print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {download.suggested_filename}")
                    await download.save_as(f"/tmp/{download.suggested_filename}")
                    print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {download.suggested_filename}")
                
                page.on('download', handle_download)
                
                await download_link.click()
                
                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
                await asyncio.sleep(3)
                
                # ë‹¤ìš´ë¡œë“œ í›„ ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
                after_count = len(self.network_requests)
                new_requests = self.network_requests[before_count:after_count]
                
                print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìƒˆë¡œìš´ ìš”ì²­ {len(new_requests)}ê°œ:")
                for req in new_requests:
                    print(f"  - {req['method']} {req['url']}")
                    if req['post_data']:
                        print(f"    POST ë°ì´í„°: {req['post_data']}")
                
            else:
                print("âŒ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _analyze_results(self):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š IRIS ì‚¬ì´íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        # 1. ì¿ í‚¤ ì •ë³´ ë¶„ì„
        print("\n1ï¸âƒ£ ì¿ í‚¤ ì •ë³´:")
        for cookie in self.cookies:
            if cookie['name'] in ['JSESSIONID', 'WMONID', 'SESSION']:
                print(f"  - {cookie['name']}: {cookie['value']}")
        
        # 2. ì£¼ìš” ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
        print("\n2ï¸âƒ£ ì£¼ìš” ë„¤íŠ¸ì›Œí¬ ìš”ì²­:")
        download_requests = [req for req in self.network_requests 
                           if 'download' in req['url'].lower() or 'atchFile' in req['url']]
        
        if download_requests:
            for req in download_requests:
                print(f"  URL: {req['url']}")
                print(f"  Method: {req['method']}")
                print(f"  Headers: {json.dumps(req['headers'], indent=4, ensure_ascii=False)}")
                if req['post_data']:
                    print(f"  POST Data: {req['post_data']}")
                print()
        else:
            print("  âŒ ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # 3. JavaScript í•¨ìˆ˜ ë¶„ì„
        print("\n3ï¸âƒ£ JavaScript ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ë¶„ì„:")
        print("  - í•¨ìˆ˜ëª…: f_bsnsAncm_downloadAtchFile")
        print("  - ì˜ˆìƒ íŒŒë¼ë¯¸í„°: atchFileId, atchFileSn")
        
        # 4. ì¶”ì²œ êµ¬í˜„ ë°©ë²•
        print("\n4ï¸âƒ£ ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê¶Œì¥ì‚¬í•­:")
        print("  1. ì„¸ì…˜ ìœ ì§€ë¥¼ ìœ„í•´ JSESSIONID ì¿ í‚¤ ë³´ì¡´")
        print("  2. ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ í›„ ë‹¤ìš´ë¡œë“œ ë§í¬ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ")
        print("  3. JavaScript í•¨ìˆ˜ ëŒ€ì‹  ì§ì ‘ ë‹¤ìš´ë¡œë“œ URL í˜¸ì¶œ")
        print("  4. Referer í—¤ë” ì„¤ì • í•„ìˆ˜")
        
        # 5. íŒŒì¼ ì €ì¥
        analysis_data = {
            'cookies': self.cookies,
            'network_requests': self.network_requests,
            'network_responses': self.network_responses,
            'timestamp': time.time()
        }
        
        with open('/tmp/iris_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ /tmp/iris_analysis.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def main():
    analyzer = IrisDownloadAnalyzer()
    await analyzer.analyze_download_mechanism()


if __name__ == "__main__":
    asyncio.run(main())