#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì „ì²´ main.py í…ŒìŠ¤íŠ¸ (6ê°œ ìŠ¤í¬ë˜í¼)
"""

import os
import sys
import logging
import concurrent.futures
from datetime import datetime
import time

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# í…ŒìŠ¤íŠ¸ìš© Enhanced ìŠ¤í¬ë˜í¼ë“¤ import (6ê°œë§Œ)
from enhanced_gsif_scraper import EnhancedGSIFScraper
from enhanced_jbf_scraper import EnhancedJBFScraper
from enhanced_koema_scraper import EnhancedKOEMAScraper
from enhanced_cci_scraper import EnhancedCCIScraper
from enhanced_dcb_scraper import EnhancedDCBScraper
from enhanced_djbea_scraper import EnhancedDJBEAScraper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

# í…ŒìŠ¤íŠ¸ìš© 6ê°œ ìŠ¤í¬ë˜í¼ ì •ì˜
TEST_SCRAPERS = {
    'gsif': {
        'class': EnhancedGSIFScraper,
        'name': 'GSIF (ê°•ë¦‰ê³¼í•™ì‚°ì—…ì§„í¥ì›)',
        'output_dir': 'gsif_enhanced'
    },
    'jbf': {
        'class': EnhancedJBFScraper,
        'name': 'JBF (ì „ë‚¨ë°”ì´ì˜¤ì§„í¥ì›)',
        'output_dir': 'jbf_enhanced'
    },
    'koema': {
        'class': EnhancedKOEMAScraper,
        'name': 'KOEMA (í•œêµ­ì—ë„ˆì§€ê³µë‹¨)',
        'output_dir': 'koema_enhanced'
    },
    'cci': {
        'class': EnhancedCCIScraper,
        'name': 'CCI (ì°½ì¡°ê²½ì œí˜ì‹ ì„¼í„°)',
        'output_dir': 'cci_enhanced'
    },
    'dcb': {
        'class': EnhancedDCBScraper,
        'name': 'DCB (ëŒ€êµ¬ë””ì§€í„¸ì‚°ì—…ì§„í¥ì›)',
        'output_dir': 'dcb_enhanced'
    },
    'djbea': {
        'class': EnhancedDJBEAScraper,
        'name': 'DJBEA (ëŒ€ì „ë°”ì´ì˜¤ì§„í¥ì›)',
        'output_dir': 'djbea_enhanced'
    }
}

def run_single_scraper(scraper_config, max_pages=1):
    """ë‹¨ì¼ ìŠ¤í¬ë˜í¼ ì‹¤í–‰"""
    scraper_key = scraper_config['key']
    scraper_info = scraper_config['info']
    
    start_time = time.time()
    
    try:
        logger.info(f"ğŸš€ [{scraper_key.upper()}] {scraper_info['name']} ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        
        # ìŠ¤í¬ë˜í¼ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        scraper_class = scraper_info['class']
        scraper = scraper_class()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = f"./output/{scraper_info['output_dir']}"
        
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        scraper.scrape_pages(max_pages=max_pages, output_base=output_dir)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ê²°ê³¼ í†µê³„ ìˆ˜ì§‘
        stats = collect_scraper_stats(output_dir)
        
        logger.info(f"âœ… [{scraper_key.upper()}] ì™„ë£Œ - {duration:.1f}ì´ˆ, ê³µê³  {stats['announcements']}ê°œ, íŒŒì¼ {stats['files']}ê°œ")
        
        return {
            'scraper': scraper_key,
            'name': scraper_info['name'],
            'status': 'success',
            'duration': duration,
            'announcements': stats['announcements'],
            'files': stats['files'],
            'total_size': stats['total_size'],
            'output_dir': output_dir
        }
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        logger.error(f"âŒ [{scraper_key.upper()}] ì‹¤íŒ¨ - {e}")
        
        return {
            'scraper': scraper_key,
            'name': scraper_info['name'],
            'status': 'error',
            'error': str(e),
            'duration': duration,
            'announcements': 0,
            'files': 0,
            'total_size': 0
        }

def collect_scraper_stats(output_dir):
    """ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ê²°ê³¼ í†µê³„ ìˆ˜ì§‘"""
    stats = {
        'announcements': 0,
        'files': 0,
        'total_size': 0
    }
    
    try:
        if not os.path.exists(output_dir):
            return stats
        
        # ê³µê³  í´ë”ë“¤ ì°¾ê¸°
        announcement_folders = [
            item for item in os.listdir(output_dir)
            if os.path.isdir(os.path.join(output_dir, item)) and 
               any(item.startswith(prefix) for prefix in ['001_', '002_', '003_', '004_', '005_', '006_', '007_', '008_', '009_'])
        ]
        
        stats['announcements'] = len(announcement_folders)
        
        # ì²¨ë¶€íŒŒì¼ í†µê³„
        for folder in announcement_folders:
            attachments_dir = os.path.join(output_dir, folder, 'attachments')
            if os.path.exists(attachments_dir):
                for file in os.listdir(attachments_dir):
                    file_path = os.path.join(attachments_dir, file)
                    if os.path.isfile(file_path):
                        stats['files'] += 1
                        stats['total_size'] += os.path.getsize(file_path)
        
    except Exception as e:
        logger.error(f"í†µê³„ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return stats

def run_scrapers_batch(scraper_batch, max_pages=1):
    """ìŠ¤í¬ë˜í¼ ë°°ì¹˜ ì‹¤í–‰ (ìµœëŒ€ 3ê°œ ë™ì‹œ)"""
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_scraper = {
            executor.submit(run_single_scraper, config, max_pages): config['key']
            for config in scraper_batch
        }
        
        results = []
        
        for future in concurrent.futures.as_completed(future_to_scraper):
            scraper_key = future_to_scraper[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"ìŠ¤í¬ë˜í¼ {scraper_key} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                results.append({
                    'scraper': scraper_key,
                    'status': 'exception',
                    'error': str(e),
                    'duration': 0,
                    'announcements': 0,
                    'files': 0,
                    'total_size': 0
                })
        
        return results

def format_size(size_bytes):
    """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f} {size_names[i]}"

def test_main_pattern():
    """main.py íŒ¨í„´ í…ŒìŠ¤íŠ¸ (6ê°œ ìŠ¤í¬ë˜í¼)"""
    print("ğŸš€ Enhanced ìŠ¤í¬ë˜í¼ í†µí•© ì‹¤í–‰ê¸° í…ŒìŠ¤íŠ¸ (6ê°œ)")
    print("="*60)
    
    start_time = datetime.now()
    
    # ìŠ¤í¬ë˜í¼ ì„¤ì • ì¤€ë¹„
    scraper_configs = []
    for key, info in TEST_SCRAPERS.items():
        scraper_configs.append({
            'key': key,
            'info': info
        })
    
    print(f"ğŸ“‹ ì´ {len(scraper_configs)}ê°œ Enhanced ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ì˜ˆì •")
    print("   3ê°œì”© ë™ì‹œ ì‹¤í–‰í•˜ì—¬ ì „ì²´ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰")
    print()
    
    # 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batch_size = 3
    all_results = []
    
    for i in range(0, len(scraper_configs), batch_size):
        batch = scraper_configs[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        
        print(f"ğŸ”„ ë°°ì¹˜ {batch_num} ì‹¤í–‰ ì¤‘ ({len(batch)}ê°œ ìŠ¤í¬ë˜í¼):")
        for config in batch:
            print(f"   â€¢ {config['info']['name']}")
        print()
        
        # ë°°ì¹˜ ì‹¤í–‰
        batch_results = run_scrapers_batch(batch, max_pages=1)
        all_results.extend(batch_results)
        
        print(f"âœ¨ ë°°ì¹˜ {batch_num} ì™„ë£Œ\n")
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # ê²°ê³¼ ìš”ì•½
    print("="*80)
    print("ğŸ¯ Enhanced ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    successful = [r for r in all_results if r['status'] == 'success']
    failed = [r for r in all_results if r['status'] in ['error', 'exception']]
    
    total_announcements = sum(r['announcements'] for r in successful)
    total_files = sum(r['files'] for r in successful)
    total_size = sum(r['total_size'] for r in successful)
    
    print(f"ğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  â€¢ ì´ ìŠ¤í¬ë˜í¼: {len(all_results)}ê°œ")
    print(f"  â€¢ ì„±ê³µ: {len(successful)}ê°œ")
    print(f"  â€¢ ì‹¤íŒ¨: {len(failed)}ê°œ")
    print(f"  â€¢ ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
    print(f"  â€¢ ì´ ìˆ˜ì§‘ ê³µê³ : {total_announcements}ê°œ")
    print(f"  â€¢ ì´ ë‹¤ìš´ë¡œë“œ íŒŒì¼: {total_files}ê°œ")
    print(f"  â€¢ ì´ íŒŒì¼ í¬ê¸°: {format_size(total_size)}")
    
    if successful:
        print(f"\nâœ… ì„±ê³µí•œ ìŠ¤í¬ë˜í¼ë“¤:")
        for result in successful:
            print(f"  â€¢ {result['name']}: {result['announcements']}ê°œ ê³µê³ , {result['files']}ê°œ íŒŒì¼, {result['duration']:.1f}ì´ˆ")
    
    if failed:
        print(f"\nâŒ ì‹¤íŒ¨í•œ ìŠ¤í¬ë˜í¼ë“¤:")
        for result in failed:
            print(f"  â€¢ {result['name']}: {result.get('error', 'Unknown error')}")
    
    print("\n" + "="*80)
    print(f"â° ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_duration:.1f}ì´ˆ")
    print(f"ğŸ“… ì‹¤í–‰ ì™„ë£Œ ì‹œê°: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    test_main_pattern()
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")