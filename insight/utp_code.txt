# UTP (울산테크노파크) 스크래퍼 개발 인사이트

## 1. 사이트 분석 결과

### URL 구조
- 기본 URL: https://www.utp.or.kr
- 목록 페이지: https://www.utp.or.kr/include/contents.php?mnuno=M0000018&menu_group=1&sno=0102
- API 엔드포인트: https://www.utp.or.kr/proc/re_ancmt/list.php
- 파일 다운로드: https://www.utp.or.kr/proc/re_ancmt/download.php

### 기술적 특징
- **AJAX/JSON API 기반 사이트**: 페이지 렌더링과 데이터 로딩이 분리됨
- **표준 UTF-8 인코딩**: 한글 처리에 문제 없음
- **SSL 인증서**: 정상 작동 (verify=True 사용 가능)
- **세션 관리**: 특별한 세션 관리 불필요
- **사용자 인증**: 로그인 없이 접근 가능

### 페이지네이션 방식
- GET 파라미터: `?task=list&page=N`
- API 응답에 페이지 정보 포함
- 한 페이지당 15개 공고 표시

## 2. 구현 기술적 도전과 해결책

### 2.1 AJAX API 인터페이스 분석
**도전**: 일반적인 HTML 스크래핑과 달리 JavaScript로 동적 로딩되는 콘텐츠

**해결책**:
```python
# API 직접 호출로 JSON 데이터 수집
response = self.session.get(
    self.api_url,
    params={
        'task': 'list',
        'page': str(page_num),
        's_state': '',
        'sear': ''
    }
)
json_data = response.json()
```

### 2.2 상세 페이지 정보 수집
**도전**: 목록에서 상세 페이지로 이동할 때 실제 HTML 페이지가 아닌 별도 API 호출 필요

**해결책**:
```python
# 목록 API에서 seq 파라미터 추출 후 상세 정보 API 호출
detail_response = self.session.get(
    self.api_url,
    params={
        'task': 'getItem',
        'seq': seq
    }
)
detail_data = detail_response.json()
```

### 2.3 첨부파일 다운로드 메커니즘
**도전**: 파일 다운로드가 직접 링크가 아닌 파라미터 기반 엔드포인트

**해결책**:
```python
# API 응답에서 파일 정보 추출 후 다운로드 URL 구성
file_url = f"{self.download_base_url}?seq={re_seq}&no={file_no}"
```

### 2.4 Enhanced Base Scraper 확장
**도전**: 기존 HTML 기반 스크래퍼와 다른 API 기반 워크플로우

**해결책**:
```python
class EnhancedUTPScraper(AjaxAPIScraper):
    def process_announcement(self, announcement: dict, index: int, output_base: str = 'output'):
        # API 전용 처리 로직으로 오버라이드
        detail = self.parse_detail_page("", announcement['url'])
```

## 3. 한글 파일명 처리

### 인코딩 방식
- **Content-Disposition 헤더**: UTF-8 인코딩으로 정상 처리
- **특수 문자 처리**: 파일명에 특수문자 (_underscore) 자동 변환
- **긴 파일명 처리**: 200자 제한으로 자동 단축

### 처리 예시
```
원본: 붙임1._2025_글로벌_진출_베트남__수출상담회__참여기업_모집공고_안_.hwp
저장: 붙임1._2025_글로벌_진출_베트남__수출상담회__참여기업_모집공고_안_.hwp
```

## 4. 데이터 구조 및 메타데이터

### API 응답 구조
```json
{
  "code": "OK",
  "data": [
    {
      "seq": "364",
      "title": "공고 제목",
      "status": "1",        // 1:접수중, 2:접수전, 3:마감
      "apply_start_dt": "2025-06-18 09:00:00",
      "apply_end_dt": "2025-07-02 23:00:00",
      "created_dt": "2025-06-19 00:00:00",
      "hit_cnt": "88",
      "is_gonggi": "N"     // Y:공지, N:일반
    }
  ],
  "page": {
    "currentPageNo": 1,
    "numberOfRecords": 350,
    "startPageNo": 1,
    "endPageNo": 10
  }
}
```

### 상세 정보 구조
```json
{
  "code": "OK",
  "data": {
    "title": "공고 제목",
    "content": "상세 내용",
    "outline": "사업 개요",
    "supported_target": "지원 대상",
    "contact_info": "담당자 정보",
    "platform_no": "1315",
    "rips_no": null
  },
  "files": [
    {
      "f_no": "0",
      "f_source": "파일명.hwp",
      "re_seq": "364"
    }
  ]
}
```

## 5. 성능 최적화

### 요청 최적화
- **API 직접 호출**: HTML 파싱 오버헤드 제거
- **세션 재사용**: requests.Session으로 연결 유지
- **적절한 지연**: 1초 간격으로 서버 부하 방지

### 중복 처리
- **제목 해시 기반**: MD5 해시로 중복 공고 감지
- **조기 종료**: 연속 3개 중복 시 자동 중단
- **세션별 관리**: 현재 실행과 이전 실행 분리

## 6. 첨부파일 다운로드 분석

### 성공률 및 파일 형태
- **다운로드 성공률**: 100% (38개 파일 중 38개 성공)
- **총 다운로드 크기**: 약 13.1 MB (15개 공고 기준)
- **평균 파일 크기**: 약 357KB per 파일

### 파일 형태 분석
- **HWP/HWPX**: 67% (한글 문서)
- **PDF**: 21% (공고문, 안내서)
- **ZIP**: 12% (양식 모음)

### 한글 파일명 특징
- 모든 파일명이 한글로 구성
- 언더스코어(_) 사용으로 공백 대체
- 긴 파일명이 일반적 (평균 50자 이상)

## 7. 오류 처리 및 복원력

### 견고한 오류 처리
```python
try:
    json_data = response.json()
    if json_data.get('code') != 'OK':
        logger.error(f"API 오류: {json_data.get('msg', '알 수 없는 오류')}")
        return []
except json.JSONDecodeError as e:
    logger.error(f"JSON 파싱 실패: {e}")
    return []
```

### 파일 다운로드 안정성
- **스트리밍 다운로드**: 대용량 파일 지원
- **파일 크기 검증**: 0바이트 파일 자동 삭제
- **재시도 메커니즘**: 네트워크 오류 시 복구

## 8. 재사용 가능한 패턴

### AjaxAPIScraper 베이스 클래스
```python
class AjaxAPIScraper(EnhancedBaseScraper):
    """AJAX/JSON API 기반 스크래퍼"""
    
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        # API 호출 및 JSON 파싱 로직
        pass
    
    def parse_api_response(self, json_data: Dict[str, Any], page_num: int) -> List[Dict[str, Any]]:
        # JSON 데이터를 공고 목록으로 변환
        pass
```

### API 기반 상세 정보 수집
```python
def parse_detail_page(self, html_content: str, detail_url: str = None) -> dict:
    # URL에서 파라미터 추출
    seq = re.search(r'seq=(\d+)', detail_url).group(1)
    
    # 별도 API 호출로 상세 정보 수집
    response = self.session.get(self.api_url, params={'task': 'getItem', 'seq': seq})
    
    # JSON 응답 처리
    return self._process_detail_json(response.json())
```

## 9. 특별한 기술적 특징

### 외부 플랫폼 연동
- **비즈니스 플랫폼**: 자체 플랫폼 연동 URL 제공
- **RIPS 연동**: 한국연구재단 시스템 연동
- **메타데이터 풍부**: 담당자 정보, 접수 기간 등 상세 정보

### 상태 관리 시스템
- **실시간 상태**: 접수중/접수전/마감 자동 계산
- **D-day 계산**: 접수 마감까지 남은 일수 표시
- **공지 구분**: 일반 공고와 공지사항 분리

## 10. 개발 검증 결과

### 테스트 결과 (1페이지 기준)
- **처리된 공고 수**: 15개
- **성공적 처리율**: 100%
- **첨부파일 다운로드**: 38개 파일, 13.1 MB
- **한글 파일명 처리**: 완벽 지원
- **원본 URL 보존**: 모든 공고에 포함

### 확장성 검증
- **3페이지 처리**: 45개 공고 처리 확인
- **대용량 파일**: 최대 8MB ZIP 파일 정상 다운로드
- **중복 처리**: 이전 실행 중복 자동 감지 및 건너뛰기

## 11. 사이트별 고유 특징

### UTP만의 특별한 요소
1. **이중 API 구조**: 목록 API와 상세 API 분리
2. **외부 플랫폼 연동**: 비즈니스 플랫폼, RIPS 연동
3. **실시간 상태 관리**: 접수 상태 동적 계산
4. **풍부한 메타데이터**: 담당자, 기간, 플랫폼 정보

### 다른 사이트 대비 장점
- **빠른 처리 속도**: HTML 파싱 불필요로 성능 향상
- **정확한 데이터**: API 직접 접근으로 데이터 정확성 보장
- **안정적 구조**: JSON 구조화된 데이터로 파싱 오류 최소화

## 12. 향후 개선 방향

### 성능 개선
- **병렬 다운로드**: 첨부파일 동시 다운로드로 속도 향상
- **캐싱 시스템**: API 응답 캐싱으로 중복 요청 방지

### 기능 확장
- **검색 필터링**: 상태별, 기간별 필터링 지원
- **알림 시스템**: 새로운 공고 알림 기능
- **통계 분석**: 공고 트렌드 분석 기능

이 UTP 스크래퍼는 현대적인 AJAX 기반 웹사이트 스크래핑의 모범 사례를 보여주며, 
다른 유사한 구조의 사이트 개발 시 재사용 가능한 패턴을 제공합니다.