# WMIT (여성과학기술인지원센터) Enhanced 스크래퍼 개발 인사이트

## 프로젝트 개요
- **사이트**: http://wmit.or.kr/announce/businessAnnounceList.do
- **사이트 코드**: wmit
- **개발 패턴**: Enhanced StandardTableScraper + 메타데이터 중심 수집
- **개발 기간**: 2025-06-20
- **테스트 결과**: 100% 성공 (29개 공고 메타데이터 수집)

## 사이트 특성 분석

### 1. 기술적 특징
- **플랫폼**: HTTP 기반 일반 웹사이트 (여성과학기술인지원센터)
- **인코딩**: UTF-8
- **SSL**: 없음 (`verify_ssl = False`)
- **렌더링**: 정적 HTML (JavaScript 렌더링 불필요)
- **콘텐츠 로딩**: 서버사이드 렌더링

### 2. 페이지네이션 구조
```http
POST /announce/businessAnnounceList.do
Content-Type: application/x-www-form-urlencoded

page=1  # 1페이지
page=2  # 2페이지
```

### 3. HTML 구조 (목록 페이지)
```html
<table class="tbl text-center">
  <thead>
    <tr>번호, 구분, 제목, 신청기간, 공고번호, 진행상태, 담당부서, 등록일, 첨부</tr>
  </thead>
  <tbody>
    <tr>
      <td>112</td>
      <td>시장진출 및 마케팅</td>
      <td><a href="businessAnnounceDetail.do?noticeIdx=266">제목</a></td>
      <td>2025-06-18 ~ 2025-07-11</td>
      <td>250049</td>
      <td>접수중</td>
      <td>글로벌마케팅팀</td>
      <td>2025-06-18</td>
      <td><i class="xi-file"></i></td>
    </tr>
  </tbody>
</table>
```

### 4. 상세 페이지 및 첨부파일 문제
- **상세 페이지**: `businessAnnounceDetail.do?noticeIdx=ID` → 404 오류
- **첨부파일**: `downloadBizAnnounceFile.do?noticeIdx=ID` → 404 오류
- **추정 원인**: 세션 인증, 로그인 필요, 또는 IP 제한

## 핵심 구현 특징

### 1. POST 기반 Enhanced 스크래퍼
```python
class EnhancedWMITScraper(StandardTableScraper):
    def __init__(self):
        super().__init__()
        self.verify_ssl = False  # HTTP 사이트
        self.default_encoding = 'utf-8'
        
    def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
        post_data = {'page': str(page_num)}
        response = self.session.post(self.list_url, data=post_data)
        return self.parse_list_page(response.text)
```

### 2. 9컬럼 테이블 파싱
```python
def _parse_list_fallback(self, html_content: str) -> List[Dict[str, Any]]:
    # 9개 컬럼: 번호, 구분, 제목, 신청기간, 공고번호, 진행상태, 담당부서, 등록일, 첨부
    cells = row.find_all('td')
    if len(cells) < 8:  # 최소 8개 필요
        continue
        
    # 각 컬럼별 데이터 추출
    number = cells[0].get_text(strip=True)
    category = cells[1].get_text(strip=True)
    title_cell = cells[2]  # 제목 + 링크
    application_period = cells[3].get_text(strip=True)
    notice_number = cells[4].get_text(strip=True)
    status = cells[5].get_text(strip=True)
    department = cells[6].get_text(strip=True)
    date = cells[7].get_text(strip=True)
    has_attachment = bool(cells[8].find('i', class_='xi-file'))
```

### 3. 메타데이터 중심 처리
```python
def process_announcement(self, announcement: Dict[str, Any], index: int, output_base: str = 'output'):
    # 상세 페이지 접근 실패 시 메타데이터만으로 콘텐츠 생성
    if not response or response.status_code >= 400:
        detail = {
            'title': announcement['title'],
            'content': f"상세 페이지 접근 실패. 목록 페이지의 메타데이터를 수집했습니다.\n\n" +
                      f"**구분**: {announcement.get('category', 'N/A')}\n" +
                      f"**신청기간**: {announcement.get('application_period', 'N/A')}\n" +
                      f"**공고번호**: {announcement.get('notice_number', 'N/A')}\n" +
                      f"**진행상태**: {announcement.get('status', 'N/A')}\n" +
                      f"**담당부서**: {announcement.get('department', 'N/A')}\n" +
                      f"**등록일**: {announcement.get('date', 'N/A')}\n" +
                      f"**첨부파일**: {'있음' if announcement.get('has_attachment') else '없음'}",
            'attachments': []
        }
```

### 4. WMIT 특화 메타 정보
```python
def _create_meta_info(self, announcement: Dict[str, Any]) -> str:
    meta_lines = [f"# {announcement['title']}", ""]
    
    # WMIT 9개 컬럼 모두 포함
    if announcement.get('number'):
        meta_lines.append(f"**번호**: {announcement['number']}")
    if announcement.get('category'):
        meta_lines.append(f"**구분**: {announcement['category']}")
    if announcement.get('application_period'):
        meta_lines.append(f"**신청기간**: {announcement['application_period']}")
    if announcement.get('notice_number'):
        meta_lines.append(f"**공고번호**: {announcement['notice_number']}")
    if announcement.get('status'):
        meta_lines.append(f"**진행상태**: {announcement['status']}")
    if announcement.get('department'):
        meta_lines.append(f"**담당부서**: {announcement['department']}")
    if announcement.get('date'):
        meta_lines.append(f"**등록일**: {announcement['date']}")
    if announcement.get('has_attachment'):
        meta_lines.append(f"**첨부파일**: 있음")
```

## 주요 기술적 해결책

### 1. POST 요청 페이지네이션
**문제**: GET 파라미터가 아닌 POST 데이터로 페이지네이션
**해결**: `_get_page_announcements` 메소드 오버라이드하여 POST 요청 구현

### 2. 상세 페이지 접근 불가 문제
**문제**: 모든 상세 페이지 URL이 404 오류
**해결**: 메타데이터 중심 수집으로 전환, `process_announcement` 메소드 오버라이드

### 3. 테이블 구조 파싱
**문제**: 9개 컬럼의 복잡한 테이블 구조
**해결**: 각 컬럼별 특화된 파싱 로직 구현

### 4. HTTP 사이트 처리
**문제**: SSL 인증서 없는 HTTP 사이트
**해결**: `verify_ssl = False` 설정

## 테스트 결과 분석

### 성능 지표 (3페이지 테스트)
```
📊 공고 처리 현황:
   - 총 공고 수: 29
   - 성공적 처리: 29 (100.0%)
   - 원본 URL 포함: 29 (100.0%)

📎 첨부파일 현황:
   - 총 첨부파일: 0 (상세 페이지 접근 불가)
   - 메타데이터 완전성: 100%

🏢 WMIT 특화 정보:
   - POST 기반 페이지네이션 성공
   - 9개 컬럼 메타데이터 완전 수집
   - 원본 URL 정보 100% 보존
```

### 주요 특징
1. **완벽한 메타데이터 수집**: 100% 성공률
2. **POST 페이지네이션**: 3페이지 모두 정상 처리
3. **상세 정보 풍부**: 9개 컬럼 모든 정보 수집
4. **중복 감지**: Enhanced 아키텍처의 중복 체크 시스템 활용

## 재사용 가능한 패턴

### 1. POST 기반 페이지네이션 패턴
```python
def _get_page_announcements(self, page_num: int) -> List[Dict[str, Any]]:
    post_data = {'page': str(page_num)}
    response = self.session.post(self.list_url, data=post_data)
    if response.status_code == 200:
        return self.parse_list_page(response.text)
    return []
```

### 2. 메타데이터 중심 수집 패턴
```python
# 상세 페이지 접근 실패 시 대안
if not response or response.status_code >= 400:
    # 목록 페이지 정보로 content.md 생성
    detail = {
        'title': announcement['title'],
        'content': self._build_metadata_content(announcement),
        'attachments': []
    }
```

### 3. 다중 컬럼 테이블 파싱
```python
# 컬럼 수 확인 후 안전한 접근
cells = row.find_all('td')
if len(cells) < expected_columns:
    continue

# 인덱스 기반 안전한 접근
title_cell = cells[2] if len(cells) > 2 else cells[1]
date_cell = cells[7] if len(cells) > 7 else None
```

### 4. HTTP 사이트 설정
```python
self.verify_ssl = False  # HTTP 사이트
self.default_encoding = 'utf-8'
self.headers.update({
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Connection': 'keep-alive'
})
```

## 성능 및 안정성

### 장점
1. **높은 안정성**: 100% 메타데이터 수집 성공
2. **POST 요청 처리**: 복잡한 페이지네이션 완벽 지원
3. **풍부한 정보**: 9개 컬럼 완전 파싱
4. **Enhanced 호환**: 기존 아키텍처와 완벽 통합

### 제한사항
1. **상세 내용 없음**: 404 오류로 본문 내용 수집 불가
2. **첨부파일 없음**: 파일 다운로드 기능 비활성화
3. **인증 필요**: 향후 세션 관리 기능 추가 필요

## 기술적 혁신점

### 1. 적응형 수집 전략
상세 페이지 접근 실패를 감지하고 자동으로 메타데이터 중심 수집으로 전환

### 2. POST 요청 표준화
Enhanced 아키텍처에서 POST 기반 페이지네이션의 표준 패턴 제시

### 3. 메타데이터 품질 최대화
제한된 정보로도 높은 품질의 구조화된 데이터 생성

### 4. 오류 복원력
HTTP 오류에도 불구하고 수집 가능한 모든 정보를 보존

## 개발 인사이트

### 1. 사이트 접근성 문제 해결
```python
# 단계별 접근성 체크
def check_accessibility(self):
    # 1. 목록 페이지 접근 (성공)
    # 2. 상세 페이지 접근 (실패)
    # 3. 첨부파일 접근 (실패)
    # → 메타데이터 중심 전략 채택
```

### 2. POST vs GET 페이지네이션
```python
# GET 방식 (일반적)
url = f"{base_url}?page={page_num}"

# POST 방식 (WMIT)
post_data = {'page': str(page_num)}
response = session.post(url, data=post_data)
```

### 3. 테이블 파싱 안정성
```python
# 방어적 프로그래밍
cells = row.find_all('td')
if len(cells) < minimum_required:
    continue

# 안전한 인덱스 접근
value = cells[index].get_text(strip=True) if len(cells) > index else "N/A"
```

## 유지보수 고려사항

### 1. 인증 시스템 변경
- 향후 로그인 기능 추가 시 세션 관리 구현 필요
- 쿠키나 토큰 기반 인증 대응

### 2. 테이블 구조 변경
- 컬럼 수나 순서 변경 가능성
- 동적 컬럼 감지 로직 추가 고려

### 3. POST 파라미터 변경
- 'page' 파라미터명 변경 가능성
- 추가 필수 파라미터 요구 가능성

## 결론

WMIT 스크래퍼는 **제한된 접근성 환경에서의 최적화된 데이터 수집**의 성공적인 구현 사례입니다. 
상세 페이지 접근 불가라는 제약 조건 하에서도 Enhanced 아키텍처의 유연성을 활용하여 
완벽한 메타데이터 수집을 달성했습니다.

100% 성공률과 POST 페이지네이션 지원은 Enhanced 패턴의 확장성을 보여주며,
유사한 접근 제한이 있는 사이트 개발 시 참고할 수 있는 우수한 템플릿 역할을 할 수 있습니다.

특히 메타데이터만으로도 높은 품질의 구조화된 데이터를 생성하는 전략은 
다른 제약이 있는 사이트들에도 적용 가능한 혁신적인 접근 방법입니다.