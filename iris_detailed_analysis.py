#!/usr/bin/env python3
"""
IRIS ì‚¬ì´íŠ¸ ìƒì„¸ ë¶„ì„ - ê³µê³  ë°ì´í„° ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ì „ ë¶„ì„
"""

import requests
import json
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, unquote
import os


class IrisDetailedAnalyzer:
    def __init__(self):
        self.base_url = "https://www.iris.go.kr"
        self.ajax_url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituList.do"
        self.detail_url_base = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituDetailView.do"
        self.session = requests.Session()
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        
        # SSL ê²½ê³  ë¬´ì‹œ
        requests.packages.urllib3.disable_warnings()
    
    def analyze_iris_completely(self):
        """IRIS ì‚¬ì´íŠ¸ ì™„ì „ ë¶„ì„"""
        print("ğŸ” IRIS ì‚¬ì´íŠ¸ ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        # 1. ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        announcements = self._get_announcements()
        
        if not announcements:
            print("âŒ ê³µê³  ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(announcements)}ê°œ ê³µê³  ë°œê²¬")
        
        # 2. ì²« 3ê°œ ê³µê³  ìƒì„¸ ë¶„ì„
        for i, announcement in enumerate(announcements[:3]):
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ ê³µê³  {i+1} ë¶„ì„: {announcement.get('ancmTl', 'N/A')}")
            print(f"{'='*60}")
            
            self._analyze_single_announcement(announcement)
        
        # 3. ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ì¢…í•© ë¶„ì„
        self._analyze_download_mechanism()
        
        # 4. ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê°€ì´ë“œ ìƒì„±
        self._generate_scraper_guide()
    
    def _get_announcements(self):
        """ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        print("ğŸ“¡ ê³µê³  ëª©ë¡ AJAX ìš”ì²­...")
        
        ajax_data = {
            'pageIndex': '1',
            'prgmId': '',
            'srchGbnCd': 'all'
        }
        
        try:
            response = self.session.post(
                self.ajax_url,
                data=ajax_data,
                verify=False,
                timeout=30
            )
            
            if response.status_code == 200:
                json_data = response.json()
                
                if 'listBsnsAncmBtinSitu' in json_data:
                    return json_data['listBsnsAncmBtinSitu']
                else:
                    print("âŒ ê³µê³  ëª©ë¡ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(json_data.keys())}")
                    return None
            else:
                print(f"âŒ AJAX ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ AJAX ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _analyze_single_announcement(self, announcement):
        """ê°œë³„ ê³µê³  ìƒì„¸ ë¶„ì„"""
        ancm_id = announcement.get('ancmId')
        if not ancm_id:
            print("âŒ ê³µê³  IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ” ê³µê³  ID: {ancm_id}")
        print(f"ğŸ“„ ì œëª©: {announcement.get('ancmTl')}")
        print(f"ğŸ¢ ê¸°ê´€: {announcement.get('sorgnNm')}")
        print(f"ğŸ“… ê³µê³ ì¼: {announcement.get('ancmDe')}")
        
        # ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
        detail_url = f"{self.detail_url_base}?ancmId={ancm_id}"
        print(f"ğŸ”— ìƒì„¸ í˜ì´ì§€: {detail_url}")
        
        try:
            response = self.session.get(detail_url, verify=False, timeout=30)
            
            if response.status_code == 200:
                print("âœ… ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì„±ê³µ")
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # 1. í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
                self._analyze_page_structure(soup)
                
                # 2. ì²¨ë¶€íŒŒì¼ ë¶„ì„
                download_info = self._analyze_attachments(soup, ancm_id)
                
                # 3. ì‹¤ì œ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
                if download_info:
                    self._test_file_download(download_info[0])  # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ í…ŒìŠ¤íŠ¸
                
            else:
                print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _analyze_page_structure(self, soup):
        """í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
        print("\nğŸ“Š í˜ì´ì§€ êµ¬ì¡° ë¶„ì„:")
        
        # ì œëª© ì˜ì—­
        title_elements = soup.find_all(['h1', 'h2', 'h3'], class_=re.compile(r'title|head', re.I))
        if title_elements:
            print(f"  ğŸ“Œ ì œëª© ìš”ì†Œ: {len(title_elements)}ê°œ")
            for elem in title_elements[:2]:
                print(f"    - {elem.name}.{elem.get('class', '')}: {elem.get_text(strip=True)[:50]}")
        
        # ë³¸ë¬¸ ì˜ì—­
        content_selectors = [
            '.content', '.board-content', '.view-content', 
            '#content', '[class*="content"]', '.detail-content'
        ]
        
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                text_length = len(content.get_text(strip=True))
                print(f"  ğŸ“ ë³¸ë¬¸ ì˜ì—­ ({selector}): {text_length}ì")
                break
        
        # í…Œì´ë¸” êµ¬ì¡°
        tables = soup.find_all('table')
        if tables:
            print(f"  ğŸ“‹ í…Œì´ë¸”: {len(tables)}ê°œ")
            for i, table in enumerate(tables[:3]):
                rows = len(table.find_all('tr'))
                print(f"    - í…Œì´ë¸” {i+1}: {rows}í–‰")
    
    def _analyze_attachments(self, soup, ancm_id):
        """ì²¨ë¶€íŒŒì¼ ë¶„ì„"""
        print("\nğŸ“ ì²¨ë¶€íŒŒì¼ ë¶„ì„:")
        
        download_info = []
        
        # 1. onclick ì†ì„±ì—ì„œ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì°¾ê¸°
        onclick_links = soup.find_all('a', onclick=True)
        
        for link in onclick_links:
            onclick = link.get('onclick', '')
            if 'download' in onclick.lower():
                text = link.get_text(strip=True)
                print(f"  ğŸ“ ì²¨ë¶€íŒŒì¼ ë°œê²¬: {text}")
                print(f"    onclick: {onclick}")
                
                # ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                params = self._extract_download_params(onclick)
                if params:
                    download_info.append({
                        'text': text,
                        'onclick': onclick,
                        'params': params,
                        'ancm_id': ancm_id
                    })
        
        # 2. href ì†ì„±ì—ì„œ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
        href_links = soup.find_all('a', href=True)
        
        for link in href_links:
            href = link.get('href', '')
            if 'download' in href.lower() or 'file' in href.lower():
                text = link.get_text(strip=True)
                print(f"  ğŸ“ ë‹¤ìš´ë¡œë“œ ë§í¬: {text}")
                print(f"    href: {href}")
                
                download_info.append({
                    'text': text,
                    'href': href,
                    'ancm_id': ancm_id
                })
        
        # 3. ì²¨ë¶€íŒŒì¼ ê´€ë ¨ í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
        file_patterns = [
            r'ì²¨ë¶€\s*:\s*([^\n\r]+)',
            r'íŒŒì¼\s*:\s*([^\n\r]+)',
            r'(\w+\.(hwp|pdf|doc|docx|xls|xlsx))',
        ]
        
        page_text = soup.get_text()
        for pattern in file_patterns:
            matches = re.findall(pattern, page_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                print(f"  ğŸ“„ íŒŒì¼ íŒ¨í„´ ë°œê²¬: {match}")
        
        print(f"  âœ… ì´ {len(download_info)}ê°œì˜ ë‹¤ìš´ë¡œë“œ ì •ë³´ ìˆ˜ì§‘")
        return download_info
    
    def _extract_download_params(self, onclick_str):
        """ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
        patterns = [
            r"f_bsnsAncm_downloadAtchFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)",
            r"downloadAtchFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)",
            r"download\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)",
            r"f_downloadFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, onclick_str)
            if match:
                return {
                    'param1': match.group(1),
                    'param2': match.group(2),
                    'pattern': pattern
                }
        
        # ë‹¨ì¼ íŒŒë¼ë¯¸í„° íŒ¨í„´
        single_patterns = [
            r"download\s*\(\s*'([^']+)'\s*\)",
            r"downloadFile\s*\(\s*'([^']+)'\s*\)"
        ]
        
        for pattern in single_patterns:
            match = re.search(pattern, onclick_str)
            if match:
                return {
                    'param1': match.group(1),
                    'pattern': pattern
                }
        
        return None
    
    def _test_file_download(self, download_info):
        """ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸: {download_info['text']}")
        
        if 'params' in download_info:
            # JavaScript í•¨ìˆ˜ ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ
            params = download_info['params']
            
            # ë‹¤ì–‘í•œ ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´ ì‹œë„
            download_urls = [
                f"{self.base_url}/downloadAtchFile.do?atchFileId={params['param1']}&atchFileSn={params['param2']}",
                f"{self.base_url}/common/downloadAtchFile.do?atchFileId={params['param1']}&atchFileSn={params['param2']}",
                f"{self.base_url}/contents/downloadAtchFile.do?atchFileId={params['param1']}&atchFileSn={params['param2']}",
                f"{self.base_url}/iris/downloadAtchFile.do?atchFileId={params['param1']}&atchFileSn={params['param2']}"
            ]
            
        elif 'href' in download_info:
            # ì§ì ‘ href ë§í¬
            href = download_info['href']
            if href.startswith('http'):
                download_urls = [href]
            else:
                download_urls = [urljoin(self.base_url, href)]
        else:
            print("âŒ ë‹¤ìš´ë¡œë“œ URLì„ êµ¬ì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê° URL ì‹œë„
        for i, url in enumerate(download_urls):
            print(f"  {i+1}. ì‹œë„: {url}")
            
            try:
                # HEAD ìš”ì²­ìœ¼ë¡œ ë¨¼ì € í™•ì¸
                head_response = self.session.head(url, verify=False, timeout=10)
                print(f"    HEAD ì‘ë‹µ: {head_response.status_code}")
                
                if head_response.status_code == 200:
                    # Content-Disposition í—¤ë” í™•ì¸
                    content_disposition = head_response.headers.get('Content-Disposition', '')
                    content_type = head_response.headers.get('Content-Type', '')
                    content_length = head_response.headers.get('Content-Length', '')
                    
                    print(f"    Content-Type: {content_type}")
                    print(f"    Content-Length: {content_length}")
                    print(f"    Content-Disposition: {content_disposition}")
                    
                    # íŒŒì¼ëª… ì¶”ì¶œ
                    filename = self._extract_filename(content_disposition)
                    if filename:
                        print(f"    âœ… íŒŒì¼ëª…: {filename}")
                    
                    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ (ì¼ë¶€ë§Œ)
                    get_response = self.session.get(url, verify=False, timeout=10, stream=True)
                    if get_response.status_code == 200:
                        # ì²˜ìŒ 1KBë§Œ ì½ì–´ì„œ íŒŒì¼ í˜•ì‹ í™•ì¸
                        first_chunk = next(get_response.iter_content(1024), b'')
                        if first_chunk:
                            print(f"    âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ! (ì²« {len(first_chunk)} bytes í™•ì¸)")
                            
                            # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
                            file_type = self._identify_file_type(first_chunk)
                            print(f"    ğŸ“„ íŒŒì¼ íƒ€ì…: {file_type}")
                            
                            return {
                                'success': True,
                                'url': url,
                                'filename': filename,
                                'content_type': content_type,
                                'file_type': file_type,
                                'size': content_length
                            }
                    
                elif head_response.status_code == 302:
                    location = head_response.headers.get('Location', '')
                    print(f"    ğŸ”„ ë¦¬ë‹¤ì´ë ‰íŠ¸: {location}")
                    
                else:
                    print(f"    âŒ ì‹¤íŒ¨: {head_response.status_code}")
                    
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
        
        return {'success': False}
    
    def _extract_filename(self, content_disposition):
        """Content-Disposition í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ"""
        if not content_disposition:
            return None
        
        # RFC 5987 í˜•ì‹ ìš°ì„  (filename*=UTF-8''encoded_filename)
        rfc5987_match = re.search(r"filename\*=([^']*)'([^']*)'(.+)", content_disposition)
        if rfc5987_match:
            encoding, lang, encoded_filename = rfc5987_match.groups()
            try:
                return unquote(encoded_filename, encoding=encoding or 'utf-8')
            except:
                pass
        
        # ì¼ë°˜ filename íŒŒë¼ë¯¸í„°
        filename_match = re.search(r'filename[^;=\n]*=([\'"]*)(.*?)\1', content_disposition)
        if filename_match:
            filename = filename_match.group(2)
            
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            for encoding in ['utf-8', 'euc-kr', 'cp949']:
                try:
                    if encoding == 'utf-8':
                        return filename.encode('latin-1').decode('utf-8')
                    else:
                        return filename.encode('latin-1').decode(encoding)
                except:
                    continue
            
            return filename
        
        return None
    
    def _identify_file_type(self, data):
        """íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ë¡œ íŒŒì¼ íƒ€ì… ì‹ë³„"""
        if not data:
            return "Unknown"
        
        # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ ë§¤ì¹­
        signatures = {
            b'\x50\x4B\x03\x04': 'ZIP/Office',
            b'\x50\x4B\x05\x06': 'ZIP/Office',
            b'\x50\x4B\x07\x08': 'ZIP/Office',
            b'%PDF': 'PDF',
            b'\xD0\xCF\x11\xE0': 'MS Office (Old)',
            b'HWP Document File': 'HWP',
            b'\xFF\xFE': 'Unicode text',
            b'\xFE\xFF': 'Unicode text',
            b'\xEF\xBB\xBF': 'UTF-8 text'
        }
        
        for sig, file_type in signatures.items():
            if data.startswith(sig):
                return file_type
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì—¬ë¶€ í™•ì¸
        try:
            data.decode('utf-8')
            return 'Text file'
        except:
            pass
        
        return 'Binary file'
    
    def _analyze_download_mechanism(self):
        """ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ì¢…í•© ë¶„ì„"""
        print("\n" + "="*80)
        print("ğŸ”§ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ì¢…í•© ë¶„ì„")
        print("="*80)
        
        print("\n1ï¸âƒ£ ë‹¤ìš´ë¡œë“œ URL íŒ¨í„´:")
        print("  - /downloadAtchFile.do?atchFileId={id}&atchFileSn={sn}")
        print("  - /common/downloadAtchFile.do?atchFileId={id}&atchFileSn={sn}")
        print("  - /contents/downloadAtchFile.do?atchFileId={id}&atchFileSn={sn}")
        
        print("\n2ï¸âƒ£ JavaScript í•¨ìˆ˜ íŒ¨í„´:")
        print("  - f_bsnsAncm_downloadAtchFile('atchFileId', 'atchFileSn')")
        print("  - downloadAtchFile('atchFileId', 'atchFileSn')")
        
        print("\n3ï¸âƒ£ íŒŒë¼ë¯¸í„° êµ¬ì¡°:")
        print("  - atchFileId: ì²¨ë¶€íŒŒì¼ ê·¸ë£¹ ID")
        print("  - atchFileSn: ì²¨ë¶€íŒŒì¼ ìˆœë²ˆ")
        
        print("\n4ï¸âƒ£ ì„¸ì…˜ ìš”êµ¬ì‚¬í•­:")
        print("  - JSESSIONID ì¿ í‚¤ í•„ìˆ˜")
        print("  - Referer í—¤ë” ì„¤ì • ê¶Œì¥")
        print("  - User-Agent ì„¤ì • í•„ìˆ˜")
        
        print("\n5ï¸âƒ£ ì‘ë‹µ íŠ¹ì„±:")
        print("  - Content-Disposition í—¤ë”ë¡œ íŒŒì¼ëª… ì œê³µ")
        print("  - í•œê¸€ íŒŒì¼ëª…ì€ UTF-8 ë˜ëŠ” EUC-KR ì¸ì½”ë”©")
        print("  - íŒŒì¼ íƒ€ì…: HWP, PDF, DOC, XLS ë“±")
    
    def _generate_scraper_guide(self):
        """ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê°€ì´ë“œ ìƒì„±"""
        print("\n" + "="*80)
        print("ğŸ“‹ IRIS ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê°€ì´ë“œ")
        print("="*80)
        
        guide = """
1. ê³µê³  ëª©ë¡ ìˆ˜ì§‘:
   - POST /contents/retrieveBsnsAncmBtinSituList.do
   - íŒŒë¼ë¯¸í„°: pageIndex=1, prgmId='', srchGbnCd='all'
   - ì‘ë‹µ: JSON í˜•íƒœ, listBsnsAncmBtinSitu í‚¤ì— ê³µê³  ë°°ì—´

2. ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼:
   - GET /contents/retrieveBsnsAncmBtinSituDetailView.do?ancmId={ancmId}
   - ancmIdëŠ” ëª©ë¡ì—ì„œ íšë“

3. ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ:
   - HTML íŒŒì‹±ìœ¼ë¡œ onclick ì†ì„±ì—ì„œ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì°¾ê¸°
   - ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ atchFileId, atchFileSn íŒŒë¼ë¯¸í„° ì¶”ì¶œ

4. íŒŒì¼ ë‹¤ìš´ë¡œë“œ:
   - GET /downloadAtchFile.do?atchFileId={id}&atchFileSn={sn}
   - ì„¸ì…˜ ì¿ í‚¤ ìœ ì§€ í•„ìˆ˜
   - Content-Disposition í—¤ë”ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ

5. íŠ¹ìˆ˜ ê³ ë ¤ì‚¬í•­:
   - í•œê¸€ íŒŒì¼ëª… ì¸ì½”ë”© ì²˜ë¦¬ (UTF-8, EUC-KR)
   - íŒŒì¼ í™•ì¥ìë³„ ì²˜ë¦¬ (HWP, PDF ë“±)
   - ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ì„¤ì •
   - SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (verify=False)

6. ì¶”ì²œ ë¼ì´ë¸ŒëŸ¬ë¦¬:
   - requests: HTTP ìš”ì²­
   - BeautifulSoup: HTML íŒŒì‹±
   - re: ì •ê·œí‘œí˜„ì‹
   - urllib.parse: URL ì²˜ë¦¬
        """
        
        print(guide)
        
        # ê°€ì´ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        with open('/tmp/iris_scraper_guide.txt', 'w', encoding='utf-8') as f:
            f.write("IRIS ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê°€ì´ë“œ\n")
            f.write("=" * 50 + "\n")
            f.write(guide)
        
        print(f"\nğŸ“ êµ¬í˜„ ê°€ì´ë“œê°€ /tmp/iris_scraper_guide.txtì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    analyzer = IrisDetailedAnalyzer()
    analyzer.analyze_iris_completely()


if __name__ == "__main__":
    main()