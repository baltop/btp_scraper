#!/usr/bin/env python3
"""
IRIS ì‚¬ì´íŠ¸ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ë° ì‹¤ì œ ê³µê³  ì ‘ê·¼
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup


class IrisPageAnalyzer:
    def __init__(self):
        self.base_url = "https://www.iris.go.kr"
        self.list_url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituListView.do"
        
    async def analyze_page_structure(self):
        """IRIS ì‚¬ì´íŠ¸ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„"""
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=False)
            
            try:
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                
                page = await context.new_page()
                
                # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§
                requests = []
                
                async def handle_request(request):
                    requests.append({
                        'url': request.url,
                        'method': request.method,
                        'headers': dict(request.headers),
                        'post_data': request.post_data
                    })
                
                page.on('request', handle_request)
                
                print("ğŸ” IRIS ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
                await page.goto(self.list_url)
                await page.wait_for_load_state('networkidle')
                
                # í˜ì´ì§€ HTML ë‚´ìš© ì¶”ì¶œ
                html_content = await page.content()
                soup = BeautifulSoup(html_content, 'html.parser')
                
                print("ğŸ“„ í˜ì´ì§€ ì œëª©:", await page.title())
                
                # 1. ê³µê³  ëª©ë¡ í…Œì´ë¸” ì°¾ê¸°
                await self._analyze_announcement_list(page, soup)
                
                # 2. í˜ì´ì§€ ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„
                await self._analyze_page_scripts(page, soup)
                
                # 3. ì‹¤ì œ ê³µê³  ì ‘ê·¼ ì‹œë„
                await self._access_actual_announcement(page)
                
                # 4. ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
                self._analyze_network_requests(requests)
                
                print("ğŸ” ë¶„ì„ ì™„ë£Œ. 10ì´ˆ í›„ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
                await asyncio.sleep(10)
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
            finally:
                await browser.close()
    
    async def _analyze_announcement_list(self, page, soup):
        """ê³µê³  ëª©ë¡ ë¶„ì„"""
        print("\n1ï¸âƒ£ ê³µê³  ëª©ë¡ ë¶„ì„:")
        
        # ë‹¤ì–‘í•œ í…Œì´ë¸” ì„ íƒì ì‹œë„
        table_selectors = [
            'table.table-list',
            'table.list-table',
            'table[summary*="ê³µê³ "]',
            'table[summary*="ëª©ë¡"]',
            'table:has(thead)',
            'table:has(tbody)',
            'table'
        ]
        
        table = None
        for selector in table_selectors:
            table = soup.select_one(selector)
            if table:
                print(f"âœ… í…Œì´ë¸” ë°œê²¬: {selector}")
                break
        
        if not table:
            print("âŒ ê³µê³  ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ëª¨ë“  í…Œì´ë¸” í™•ì¸
            all_tables = soup.find_all('table')
            print(f"ğŸ“‹ ì´ {len(all_tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
            for i, t in enumerate(all_tables):
                print(f"  í…Œì´ë¸” {i+1}: {t.get('class', '')}, {t.get('summary', '')}")
            return
        
        # í…Œì´ë¸” í—¤ë” ë¶„ì„
        thead = table.find('thead')
        if thead:
            headers = [th.get_text(strip=True) for th in thead.find_all('th')]
            print(f"ğŸ“‹ í…Œì´ë¸” í—¤ë”: {headers}")
        
        # í…Œì´ë¸” í–‰ ë¶„ì„
        tbody = table.find('tbody')
        if tbody:
            rows = tbody.find_all('tr')
            print(f"ğŸ“‹ ê³µê³  í–‰ ìˆ˜: {len(rows)}")
            
            # ì²« ë²ˆì§¸ í–‰ ìƒì„¸ ë¶„ì„
            if rows:
                first_row = rows[0]
                cells = first_row.find_all(['td', 'th'])
                print(f"ğŸ“‹ ì²« ë²ˆì§¸ í–‰ ì…€ ìˆ˜: {len(cells)}")
                
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)[:50]
                    links = cell.find_all('a')
                    print(f"  ì…€ {i+1}: {text}")
                    for link in links:
                        href = link.get('href', '')
                        onclick = link.get('onclick', '')
                        print(f"    ë§í¬: href='{href}', onclick='{onclick}'")
    
    async def _analyze_page_scripts(self, page, soup):
        """í˜ì´ì§€ ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„"""
        print("\n2ï¸âƒ£ JavaScript í•¨ìˆ˜ ë¶„ì„:")
        
        # ëª¨ë“  ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ í™•ì¸
        scripts = soup.find_all('script')
        print(f"ğŸ“‹ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ìˆ˜: {len(scripts)}")
        
        # ë‹¤ìš´ë¡œë“œ ê´€ë ¨ í•¨ìˆ˜ ì°¾ê¸°
        download_functions = []
        for script in scripts:
            if script.string:
                content = script.string
                if 'downloadAtchFile' in content or 'download' in content.lower():
                    download_functions.append(content)
        
        if download_functions:
            print("âœ… ë‹¤ìš´ë¡œë“œ ê´€ë ¨ JavaScript í•¨ìˆ˜ ë°œê²¬:")
            for i, func in enumerate(download_functions):
                lines = func.split('\n')
                relevant_lines = [line.strip() for line in lines 
                                if 'download' in line.lower() or 'atchFile' in line]
                print(f"  í•¨ìˆ˜ {i+1}:")
                for line in relevant_lines[:10]:  # ì²˜ìŒ 10ì¤„ë§Œ ì¶œë ¥
                    print(f"    {line}")
        
        # í˜ì´ì§€ì—ì„œ JavaScript í•¨ìˆ˜ ì‹¤í–‰í•˜ì—¬ í™•ì¸
        try:
            result = await page.evaluate("""
                () => {
                    let functions = [];
                    for (let prop in window) {
                        if (typeof window[prop] === 'function' && prop.includes('download')) {
                            functions.push(prop);
                        }
                    }
                    return functions;
                }
            """)
            print(f"ğŸ“‹ ì „ì—­ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜: {result}")
        except Exception as e:
            print(f"âŒ JavaScript í•¨ìˆ˜ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _access_actual_announcement(self, page):
        """ì‹¤ì œ ê³µê³  ì ‘ê·¼ ì‹œë„"""
        print("\n3ï¸âƒ£ ì‹¤ì œ ê³µê³  ì ‘ê·¼ ì‹œë„:")
        
        # ê³µê³  ë§í¬ í´ë¦­ ì‹œë„
        try:
            # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ê³µê³  ë§í¬ ì°¾ê¸°
            selectors = [
                'a[href*="retrieveBsnsAncmBtinSituDetailView"]',
                'a[onclick*="retrieveBsnsAncmBtinSituDetailView"]',
                'tbody tr:first-child a',
                'table a:first-of-type'
            ]
            
            for selector in selectors:
                element = await page.query_selector(selector)
                if element:
                    text = await element.text_content()
                    href = await element.get_attribute('href')
                    onclick = await element.get_attribute('onclick')
                    
                    print(f"âœ… ê³µê³  ë§í¬ ë°œê²¬: {selector}")
                    print(f"  í…ìŠ¤íŠ¸: {text}")
                    print(f"  href: {href}")
                    print(f"  onclick: {onclick}")
                    
                    # ë§í¬ í´ë¦­
                    print("ğŸ”— ê³µê³  ë§í¬ í´ë¦­...")
                    await element.click()
                    await page.wait_for_load_state('networkidle')
                    
                    # ìƒì„¸ í˜ì´ì§€ ë¶„ì„
                    await self._analyze_detail_page(page)
                    return
            
            print("âŒ ê³µê³  ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ê³µê³  ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _analyze_detail_page(self, page):
        """ìƒì„¸ í˜ì´ì§€ ë¶„ì„"""
        print("\n4ï¸âƒ£ ìƒì„¸ í˜ì´ì§€ ë¶„ì„:")
        
        try:
            current_url = page.url
            print(f"ğŸ“„ í˜„ì¬ URL: {current_url}")
            
            # ì²¨ë¶€íŒŒì¼ ì˜ì—­ ì°¾ê¸°
            html_content = await page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
            attachment_selectors = [
                'a[onclick*="downloadAtchFile"]',
                'a[href*="download"]',
                'a[onclick*="download"]',
                '*[class*="attach"]',
                '*[class*="file"]'
            ]
            
            for selector in attachment_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"âœ… ì²¨ë¶€íŒŒì¼ ìš”ì†Œ ë°œê²¬: {selector} ({len(elements)}ê°œ)")
                    for i, elem in enumerate(elements[:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                        print(f"  {i+1}. {elem.get_text(strip=True)}")
                        print(f"     href: {elem.get('href', '')}")
                        print(f"     onclick: {elem.get('onclick', '')}")
            
            # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë§í¬ í´ë¦­ ì‹œë„
            download_link = await page.query_selector('a[onclick*="downloadAtchFile"]')
            if download_link:
                print("ğŸ“¥ ë‹¤ìš´ë¡œë“œ ë§í¬ í´ë¦­ ì‹œë„...")
                
                # ë‹¤ìš´ë¡œë“œ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
                download_info = []
                
                async def handle_download(download):
                    info = {
                        'filename': download.suggested_filename,
                        'url': download.url
                    }
                    download_info.append(info)
                    print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {download.suggested_filename}")
                    print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ URL: {download.url}")
                
                page.on('download', handle_download)
                
                await download_link.click()
                await asyncio.sleep(2)
                
                if download_info:
                    print("âœ… ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
                    for info in download_info:
                        print(f"  íŒŒì¼ëª…: {info['filename']}")
                        print(f"  URL: {info['url']}")
                else:
                    print("âŒ ë‹¤ìš´ë¡œë“œ ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _analyze_network_requests(self, requests):
        """ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„"""
        print("\n5ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„:")
        
        # ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­ í•„í„°ë§
        download_requests = [req for req in requests 
                           if 'download' in req['url'].lower() or 'atchFile' in req['url']]
        
        if download_requests:
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­ {len(download_requests)}ê°œ ë°œê²¬:")
            for req in download_requests:
                print(f"  URL: {req['url']}")
                print(f"  Method: {req['method']}")
                if req['post_data']:
                    print(f"  POST Data: {req['post_data']}")
        else:
            print("âŒ ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì£¼ìš” ìš”ì²­ íƒ€ì… ë¶„ì„
        post_requests = [req for req in requests if req['method'] == 'POST']
        print(f"ğŸ“‹ POST ìš”ì²­ ìˆ˜: {len(post_requests)}")
        
        for req in post_requests:
            if 'iris.go.kr' in req['url']:
                print(f"  - {req['url']}")
                if req['post_data']:
                    print(f"    Data: {req['post_data']}")


async def main():
    analyzer = IrisPageAnalyzer()
    await analyzer.analyze_page_structure()


if __name__ == "__main__":
    asyncio.run(main())