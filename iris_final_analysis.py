#!/usr/bin/env python3
"""
IRIS ì‚¬ì´íŠ¸ ìµœì¢… ë¶„ì„ - ë¸Œë¼ìš°ì € ê¸°ë°˜ ì‹¤ì œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import time


class IrisFinalAnalyzer:
    def __init__(self):
        self.base_url = "https://www.iris.go.kr"
        self.list_url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituListView.do"
        self.network_requests = []
        self.download_info = []
        
    async def final_analysis(self):
        """ìµœì¢… ì¢…í•© ë¶„ì„ - ë¸Œë¼ìš°ì € ê¸°ë°˜"""
        print("ğŸ” IRIS ì‚¬ì´íŠ¸ ìµœì¢… ë¶„ì„ ì‹œì‘...")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            try:
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                
                page = await context.new_page()
                
                # ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§
                await self._setup_network_monitoring(page)
                
                # 1. ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ
                print("ğŸŒ IRIS ì‚¬ì´íŠ¸ ì ‘ì†...")
                await page.goto(self.list_url, wait_until='networkidle')
                
                # 2. AJAXë¡œ ê³µê³  ëª©ë¡ ë¡œë“œ
                await self._load_announcements_via_ajax(page)
                
                # 3. ì²« ë²ˆì§¸ ê³µê³  í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
                await self._access_first_announcement(page)
                
                # 4. ì²¨ë¶€íŒŒì¼ ì°¾ê¸° ë° ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
                await self._test_attachment_download(page)
                
                # 5. ìµœì¢… ê²°ê³¼ ë¶„ì„
                await self._final_results_analysis()
                
                print("ğŸ” ë¶„ì„ ì™„ë£Œ. 10ì´ˆ í›„ ë¸Œë¼ìš°ì € ì¢…ë£Œ...")
                await asyncio.sleep(10)
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
            finally:
                await browser.close()
    
    async def _setup_network_monitoring(self, page):
        """ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        
        async def handle_request(request):
            self.network_requests.append({
                'url': request.url,
                'method': request.method,
                'headers': dict(request.headers),
                'post_data': request.post_data,
                'timestamp': time.time()
            })
        
        page.on('request', handle_request)
    
    async def _load_announcements_via_ajax(self, page):
        """AJAXë¡œ ê³µê³  ëª©ë¡ ë¡œë“œ"""
        print("ğŸ“¡ AJAXë¡œ ê³µê³  ëª©ë¡ ë¡œë“œ ì¤‘...")
        
        # JavaScriptë¡œ AJAX ìš”ì²­ ì‹¤í–‰
        result = await page.evaluate("""
            async () => {
                try {
                    const response = await fetch('/contents/retrieveBsnsAncmBtinSituList.do', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
                            'X-Requested-With': 'XMLHttpRequest'
                        },
                        body: 'pageIndex=1&prgmId=&srchGbnCd=all'
                    });
                    
                    if (response.ok) {
                        const data = await response.json();
                        return {
                            success: true,
                            data: data,
                            count: data.listBsnsAncmBtinSitu ? data.listBsnsAncmBtinSitu.length : 0
                        };
                    } else {
                        return {
                            success: false,
                            status: response.status,
                            statusText: response.statusText
                        };
                    }
                } catch (error) {
                    return {
                        success: false,
                        error: error.message
                    };
                }
            }
        """)
        
        if result['success']:
            print(f"âœ… AJAX ì„±ê³µ: {result['count']}ê°œ ê³µê³  ë¡œë“œ")
            return result['data']
        else:
            print(f"âŒ AJAX ì‹¤íŒ¨: {result}")
            return None
    
    async def _access_first_announcement(self, page):
        """ì²« ë²ˆì§¸ ê³µê³  ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼"""
        print("ğŸ” ì²« ë²ˆì§¸ ê³µê³  ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼...")
        
        # í˜ì´ì§€ì—ì„œ ê³µê³  ë§í¬ ì°¾ê¸° (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
        
        # 1. í…Œì´ë¸” ê¸°ë°˜ ë§í¬ ì°¾ê¸°
        links_found = False
        
        # JavaScriptë¡œ ëª¨ë“  ë§í¬ ê²€ìƒ‰
        all_links = await page.evaluate("""
            () => {
                const links = Array.from(document.querySelectorAll('a'));
                return links.map(link => ({
                    href: link.href,
                    text: link.textContent.trim(),
                    onclick: link.onclick ? link.onclick.toString() : null,
                    id: link.id,
                    className: link.className
                })).filter(link => 
                    link.text.length > 0 && 
                    (link.href.includes('Detail') || 
                     link.onclick && link.onclick.includes('Detail') ||
                     link.text.includes('ê³µê³ ') ||
                     link.text.includes('ì‚¬ì—…'))
                );
            }
        """)
        
        print(f"ğŸ“‹ ê´€ë ¨ ë§í¬ {len(all_links)}ê°œ ë°œê²¬:")
        for i, link in enumerate(all_links[:5]):
            print(f"  {i+1}. {link['text'][:50]}")
            print(f"     href: {link['href']}")
            print(f"     onclick: {link['onclick']}")
        
        # ì²« ë²ˆì§¸ ê³µê³  ë§í¬ í´ë¦­ ì‹œë„
        if all_links:
            try:
                first_link = all_links[0]
                print(f"ğŸ”— ì²« ë²ˆì§¸ ë§í¬ í´ë¦­: {first_link['text']}")
                
                # í´ë¦­ ì „ í˜„ì¬ URL ê¸°ë¡
                current_url = page.url
                
                # JavaScriptë¡œ ì§ì ‘ í´ë¦­
                await page.evaluate(f"""
                    () => {{
                        const links = Array.from(document.querySelectorAll('a'));
                        const targetLink = links.find(link => 
                            link.textContent.trim() === '{first_link['text']}'
                        );
                        if (targetLink) {{
                            targetLink.click();
                            return true;
                        }}
                        return false;
                    }}
                """)
                
                # í˜ì´ì§€ ë³€ê²½ ëŒ€ê¸°
                await page.wait_for_load_state('networkidle')
                await asyncio.sleep(2)
                
                new_url = page.url
                if new_url != current_url:
                    print(f"âœ… í˜ì´ì§€ ì´ë™ ì„±ê³µ: {new_url}")
                    return True
                else:
                    print("âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"âŒ ë§í¬ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # 2. ì§ì ‘ ìƒì„¸ í˜ì´ì§€ URL ì‹œë„
        print("ğŸ”„ ì§ì ‘ ìƒì„¸ í˜ì´ì§€ URL ì ‘ê·¼ ì‹œë„...")
        
        # ìµœê·¼ ê³µê³  IDë¡œ ì§ì ‘ ì ‘ê·¼ ì‹œë„
        test_urls = [
            "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituDetailView.do?ancmId=014116",
            "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituDetailView.do?ancmId=014114",
            "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituDetailView.do?ancmId=014079"
        ]
        
        for url in test_urls:
            try:
                print(f"ğŸ”— URL ì ‘ê·¼ ì‹œë„: {url}")
                await page.goto(url, wait_until='networkidle')
                
                # í˜ì´ì§€ ë‚´ìš© í™•ì¸
                title = await page.title()
                if '404' not in title and 'error' not in title.lower():
                    print(f"âœ… ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì„±ê³µ: {title}")
                    return True
                else:
                    print(f"âŒ í˜ì´ì§€ ì˜¤ë¥˜: {title}")
                    
            except Exception as e:
                print(f"âŒ URL ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    async def _test_attachment_download(self, page):
        """ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸:")
        
        # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        download_links = await page.query_selector_all('a[onclick*="download"], a[href*="download"]')
        
        if not download_links:
            print("âŒ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í˜ì´ì§€ HTML ë¶„ì„
            html_content = await page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
            page_text = soup.get_text()
            keywords = ['ì²¨ë¶€', 'íŒŒì¼', 'ë‹¤ìš´ë¡œë“œ', '.hwp', '.pdf', '.doc']
            
            for keyword in keywords:
                if keyword in page_text:
                    print(f"âœ… '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                    # í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    lines = page_text.split('\n')
                    for line in lines:
                        if keyword in line and line.strip():
                            print(f"  ì»¨í…ìŠ¤íŠ¸: {line.strip()[:100]}")
                            break
            
            return
        
        print(f"ğŸ“ ë‹¤ìš´ë¡œë“œ ë§í¬ {len(download_links)}ê°œ ë°œê²¬")
        
        # ì²« ë²ˆì§¸ ë‹¤ìš´ë¡œë“œ ë§í¬ ë¶„ì„
        for i, link in enumerate(download_links[:3]):
            try:
                text = await link.text_content()
                onclick = await link.get_attribute('onclick')
                href = await link.get_attribute('href')
                
                print(f"\nğŸ“ ë‹¤ìš´ë¡œë“œ ë§í¬ {i+1}: {text}")
                print(f"  onclick: {onclick}")
                print(f"  href: {href}")
                
                # ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
                if onclick:
                    params = self._extract_download_params(onclick)
                    if params:
                        print(f"  íŒŒë¼ë¯¸í„°: {params}")
                        
                        # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL í…ŒìŠ¤íŠ¸
                        await self._test_download_url(page, params)
                
            except Exception as e:
                print(f"âŒ ë§í¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _extract_download_params(self, onclick_str):
        """ë‹¤ìš´ë¡œë“œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ"""
        patterns = [
            r"f_bsnsAncm_downloadAtchFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)",
            r"downloadAtchFile\s*\(\s*'([^']+)'\s*,\s*'([^']+)'\s*\)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, onclick_str)
            if match:
                return {
                    'atchFileId': match.group(1),
                    'atchFileSn': match.group(2)
                }
        
        return None
    
    async def _test_download_url(self, page, params):
        """ë‹¤ìš´ë¡œë“œ URL í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ URL í…ŒìŠ¤íŠ¸...")
        
        download_urls = [
            f"/downloadAtchFile.do?atchFileId={params['atchFileId']}&atchFileSn={params['atchFileSn']}",
            f"/common/downloadAtchFile.do?atchFileId={params['atchFileId']}&atchFileSn={params['atchFileSn']}",
            f"/contents/downloadAtchFile.do?atchFileId={params['atchFileId']}&atchFileSn={params['atchFileSn']}"
        ]
        
        for i, url in enumerate(download_urls):
            try:
                print(f"  {i+1}. í…ŒìŠ¤íŠ¸: {url}")
                
                # JavaScriptë¡œ HEAD ìš”ì²­ ì‹œë„
                result = await page.evaluate(f"""
                    async () => {{
                        try {{
                            const response = await fetch('{url}', {{
                                method: 'HEAD'
                            }});
                            
                            return {{
                                success: true,
                                status: response.status,
                                headers: Object.fromEntries(response.headers.entries())
                            }};
                        }} catch (error) {{
                            return {{
                                success: false,
                                error: error.message
                            }};
                        }}
                    }}
                """)
                
                if result['success'] and result['status'] == 200:
                    print(f"    âœ… ì‘ë‹µ ì„±ê³µ: {result['status']}")
                    headers = result['headers']
                    
                    if 'content-disposition' in headers:
                        print(f"    ğŸ“„ Content-Disposition: {headers['content-disposition']}")
                    
                    if 'content-type' in headers:
                        print(f"    ğŸ“„ Content-Type: {headers['content-type']}")
                    
                    self.download_info.append({
                        'url': url,
                        'params': params,
                        'headers': headers,
                        'status': 'success'
                    })
                    
                    return url
                    
                else:
                    print(f"    âŒ ì‹¤íŒ¨: {result}")
                    
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
        
        return None
    
    async def _final_results_analysis(self):
        """ìµœì¢… ê²°ê³¼ ë¶„ì„"""
        print("\n" + "="*80)
        print("ğŸ“Š IRIS ì‚¬ì´íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ìµœì¢… ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        # 1. ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
        iris_requests = [req for req in self.network_requests if 'iris.go.kr' in req['url']]
        ajax_requests = [req for req in iris_requests if req['method'] == 'POST']
        
        print(f"\n1ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í†µê³„:")
        print(f"  - ì´ ìš”ì²­: {len(self.network_requests)}")
        print(f"  - IRIS ê´€ë ¨: {len(iris_requests)}")
        print(f"  - AJAX ìš”ì²­: {len(ajax_requests)}")
        
        # 2. AJAX ìš”ì²­ ë¶„ì„
        if ajax_requests:
            print(f"\n2ï¸âƒ£ AJAX ìš”ì²­ ìƒì„¸:")
            for req in ajax_requests[:5]:
                print(f"  - {req['method']} {req['url']}")
                if req['post_data']:
                    print(f"    ë°ì´í„°: {req['post_data']}")
        
        # 3. ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ìš”ì•½
        print(f"\n3ï¸âƒ£ ë‹¤ìš´ë¡œë“œ ë©”ì»¤ë‹ˆì¦˜ ìš”ì•½:")
        print("  âœ… ê³µê³  ëª©ë¡: AJAX POST ìš”ì²­ìœ¼ë¡œ JSON ë°ì´í„° íšë“")
        print("  ğŸ“¡ URL: /contents/retrieveBsnsAncmBtinSituList.do")
        print("  ğŸ“Š íŒŒë¼ë¯¸í„°: pageIndex, prgmId, srchGbnCd")
        print("  ğŸ“‹ ì‘ë‹µ: listBsnsAncmBtinSitu ë°°ì—´ì— ê³µê³  ì •ë³´")
        
        print(f"\n  ğŸ“„ ìƒì„¸ í˜ì´ì§€: ancmIdë¡œ ê°œë³„ ê³µê³  ì ‘ê·¼")
        print("  ğŸ“¡ URL: /contents/retrieveBsnsAncmBtinSituDetailView.do?ancmId={id}")
        
        print(f"\n  ğŸ“ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: JavaScript í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ")
        print("  ğŸ”§ í•¨ìˆ˜: f_bsnsAncm_downloadAtchFile(atchFileId, atchFileSn)")
        print("  ğŸ“¡ URL: /downloadAtchFile.do?atchFileId={id}&atchFileSn={sn}")
        
        # 4. ì„¸ì…˜ ë° ë³´ì•ˆ ìš”êµ¬ì‚¬í•­
        print(f"\n4ï¸âƒ£ ì„¸ì…˜ ë° ë³´ì•ˆ ìš”êµ¬ì‚¬í•­:")
        
        # ë§ˆì§€ë§‰ ìš”ì²­ì—ì„œ ì¿ í‚¤ ì •ë³´ í™•ì¸
        if self.network_requests:
            last_request = self.network_requests[-1]
            cookie_header = last_request['headers'].get('cookie', '')
            if 'JSESSIONID' in cookie_header:
                print("  âœ… JSESSIONID ì¿ í‚¤ í™•ì¸ë¨")
            else:
                print("  âŒ JSESSIONID ì¿ í‚¤ ë¯¸í™•ì¸")
        
        print("  ğŸ” í•„ìˆ˜ í—¤ë”:")
        print("    - User-Agent: ë¸Œë¼ìš°ì € ì‹ë³„")
        print("    - Referer: ìƒì„¸ í˜ì´ì§€ URL")
        print("    - Cookie: JSESSIONID ì„¸ì…˜ ìœ ì§€")
        
        # 5. êµ¬í˜„ ì˜ˆì‹œ ì½”ë“œ
        print(f"\n5ï¸âƒ£ êµ¬í˜„ ì˜ˆì‹œ:")
        
        example_code = '''
import requests
from bs4 import BeautifulSoup
import json

class IrisScraper:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://www.iris.go.kr"
        
    def get_announcements(self, page=1):
        """ê³µê³  ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/contents/retrieveBsnsAncmBtinSituList.do"
        data = {
            'pageIndex': page,
            'prgmId': '',
            'srchGbnCd': 'all'
        }
        
        response = self.session.post(url, data=data, verify=False)
        return response.json()['listBsnsAncmBtinSitu']
    
    def get_detail_page(self, ancm_id):
        """ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        url = f"{self.base_url}/contents/retrieveBsnsAncmBtinSituDetailView.do"
        params = {'ancmId': ancm_id}
        
        response = self.session.get(url, params=params, verify=False)
        return response.text
    
    def download_file(self, atch_file_id, atch_file_sn):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        url = f"{self.base_url}/downloadAtchFile.do"
        params = {
            'atchFileId': atch_file_id,
            'atchFileSn': atch_file_sn
        }
        
        response = self.session.get(url, params=params, verify=False)
        return response.content
        '''
        
        print(example_code)
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        result_data = {
            'timestamp': time.time(),
            'network_requests_count': len(self.network_requests),
            'ajax_requests': ajax_requests,
            'download_info': self.download_info,
            'mechanism_summary': {
                'list_endpoint': '/contents/retrieveBsnsAncmBtinSituList.do',
                'detail_endpoint': '/contents/retrieveBsnsAncmBtinSituDetailView.do',
                'download_endpoint': '/downloadAtchFile.do',
                'required_cookies': ['JSESSIONID'],
                'required_headers': ['User-Agent', 'Referer']
            }
        }
        
        with open('/tmp/iris_final_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ ìµœì¢… ë¶„ì„ ê²°ê³¼ê°€ /tmp/iris_final_analysis.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def main():
    analyzer = IrisFinalAnalyzer()
    await analyzer.final_analysis()


if __name__ == "__main__":
    asyncio.run(main())