#!/usr/bin/env python3
"""
IRIS ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ - AJAX ë° ë™ì  ì½˜í…ì¸  í¬í•¨
"""

import asyncio
import json
import re
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import time


class IrisComprehensiveAnalyzer:
    def __init__(self):
        self.base_url = "https://www.iris.go.kr"
        self.list_url = "https://www.iris.go.kr/contents/retrieveBsnsAncmBtinSituListView.do"
        self.all_requests = []
        self.all_responses = []
        
    async def comprehensive_analysis(self):
        """IRIS ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„"""
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=False,
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-web-security']
            )
            
            try:
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                    java_script_enabled=True
                )
                
                page = await context.new_page()
                
                # ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •
                await self._setup_comprehensive_monitoring(page)
                
                print("ğŸ” IRIS ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
                await page.goto(self.list_url, wait_until='networkidle', timeout=60000)
                
                # í˜ì´ì§€ ì™„ì „ ë¡œë“œ ëŒ€ê¸°
                await asyncio.sleep(5)
                
                # 1. í˜ì´ì§€ ê¸°ë³¸ ì •ë³´ ë¶„ì„
                await self._analyze_basic_info(page)
                
                # 2. AJAX ìš”ì²­ í™•ì¸
                await self._trigger_ajax_requests(page)
                
                # 3. ë™ì  ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°
                await self._wait_for_dynamic_content(page)
                
                # 4. ê³µê³  ëª©ë¡ ì¬ë¶„ì„
                await self._reanalyze_announcements(page)
                
                # 5. ì‹¤ì œ ê³µê³  ì ‘ê·¼ ë° íŒŒì¼ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸
                await self._test_file_download_process(page)
                
                # 6. ì¢…í•© ê²°ê³¼ ë¶„ì„
                await self._comprehensive_results_analysis()
                
                print("ğŸ” ë¶„ì„ ì™„ë£Œ. ë¸Œë¼ìš°ì €ë¥¼ 15ì´ˆ í›„ ì¢…ë£Œ...")
                await asyncio.sleep(15)
                
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                
            finally:
                await browser.close()
    
    async def _setup_comprehensive_monitoring(self, page):
        """ì¢…í•© ë„¤íŠ¸ì›Œí¬ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        
        async def handle_request(request):
            self.all_requests.append({
                'url': request.url,
                'method': request.method,
                'headers': dict(request.headers),
                'post_data': request.post_data,
                'resource_type': request.resource_type,
                'timestamp': time.time()
            })
            
        async def handle_response(response):
            self.all_responses.append({
                'url': response.url,
                'status': response.status,
                'headers': dict(response.headers),
                'timestamp': time.time()
            })
            
        page.on('request', handle_request)
        page.on('response', handle_response)
        
        # ì½˜ì†” ë©”ì‹œì§€ ëª¨ë‹ˆí„°ë§
        page.on('console', lambda msg: print(f"ğŸ–¥ï¸ Console: {msg.text}"))
    
    async def _analyze_basic_info(self, page):
        """í˜ì´ì§€ ê¸°ë³¸ ì •ë³´ ë¶„ì„"""
        print("\n1ï¸âƒ£ í˜ì´ì§€ ê¸°ë³¸ ì •ë³´:")
        
        title = await page.title()
        url = page.url
        print(f"ğŸ“„ ì œëª©: {title}")
        print(f"ğŸŒ URL: {url}")
        
        # í˜ì´ì§€ ë¡œë”© ìƒíƒœ í™•ì¸
        ready_state = await page.evaluate("document.readyState")
        print(f"ğŸ“‹ ë¬¸ì„œ ìƒíƒœ: {ready_state}")
        
        # jQuery í™•ì¸
        jquery_version = await page.evaluate("""
            () => {
                if (typeof jQuery !== 'undefined') {
                    return jQuery.fn.jquery;
                } else if (typeof $ !== 'undefined' && $.fn) {
                    return $.fn.jquery;
                }
                return null;
            }
        """)
        print(f"ğŸ“‹ jQuery ë²„ì „: {jquery_version}")
    
    async def _trigger_ajax_requests(self, page):
        """AJAX ìš”ì²­ íŠ¸ë¦¬ê±°"""
        print("\n2ï¸âƒ£ AJAX ìš”ì²­ íŠ¸ë¦¬ê±°:")
        
        # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (Lazy loading íŠ¸ë¦¬ê±°)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(2)
        
        # ê²€ìƒ‰ í¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¤í–‰
        search_form = await page.query_selector('form')
        if search_form:
            print("âœ… ê²€ìƒ‰ í¼ ë°œê²¬, ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰...")
            search_button = await page.query_selector('input[type="submit"], button[type="submit"]')
            if search_button:
                await search_button.click()
                await page.wait_for_load_state('networkidle')
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ë²„íŠ¼ í™•ì¸
        pagination_links = await page.query_selector_all('a[href*="page"], a[onclick*="page"]')
        if pagination_links:
            print(f"âœ… í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ {len(pagination_links)}ê°œ ë°œê²¬")
    
    async def _wait_for_dynamic_content(self, page):
        """ë™ì  ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°"""
        print("\n3ï¸âƒ£ ë™ì  ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°:")
        
        # í…Œì´ë¸”ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        try:
            await page.wait_for_selector('table, .list-container, .board-list', timeout=10000)
            print("âœ… ëª©ë¡ ì»¨í…Œì´ë„ˆ ë¡œë“œ ì™„ë£Œ")
        except:
            print("âŒ ëª©ë¡ ì»¨í…Œì´ë„ˆ ë¡œë“œ ì‹¤íŒ¨")
        
        # ì¶”ê°€ ëŒ€ê¸° ì‹œê°„
        await asyncio.sleep(3)
        
        # í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ìš”ì†Œ ë‹¤ì‹œ í™•ì¸
        all_elements = await page.evaluate("""
            () => {
                const elements = document.querySelectorAll('*');
                let result = {
                    total: elements.length,
                    tables: document.querySelectorAll('table').length,
                    links: document.querySelectorAll('a').length,
                    forms: document.querySelectorAll('form').length
                };
                return result;
            }
        """)
        print(f"ğŸ“‹ í˜ì´ì§€ ìš”ì†Œ: {all_elements}")
    
    async def _reanalyze_announcements(self, page):
        """ê³µê³  ëª©ë¡ ì¬ë¶„ì„"""
        print("\n4ï¸âƒ£ ê³µê³  ëª©ë¡ ì¬ë¶„ì„:")
        
        # í˜ì´ì§€ HTML ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë‹¤ì–‘í•œ ëª©ë¡ êµ¬ì¡° ì°¾ê¸°
        list_selectors = [
            'table',
            '.list-container',
            '.board-list',
            'ul.list',
            'div[class*="list"]',
            'tbody tr',
            'li:has(a)'
        ]
        
        found_lists = []
        for selector in list_selectors:
            elements = soup.select(selector)
            if elements:
                found_lists.append((selector, len(elements)))
                print(f"âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
        
        if not found_lists:
            print("âŒ ëª©ë¡ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ëª¨ë“  í…ìŠ¤íŠ¸ ë‚´ìš© ê²€ìƒ‰
            all_text = soup.get_text()
            if 'ê³µê³ ' in all_text or 'ì‚¬ì—…' in all_text:
                print("âœ… í˜ì´ì§€ì— ê³µê³  ê´€ë ¨ í…ìŠ¤íŠ¸ ë°œê²¬")
                # ê³µê³  ê´€ë ¨ í‚¤ì›Œë“œ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                lines = all_text.split('\n')
                relevant_lines = [line.strip() for line in lines 
                                if line.strip() and ('ê³µê³ ' in line or 'ì‚¬ì—…' in line)][:10]
                for line in relevant_lines:
                    print(f"  - {line}")
        
        # ë§í¬ ë¶„ì„
        all_links = soup.find_all('a', href=True)
        detail_links = [link for link in all_links 
                       if ('Detail' in link.get('href', '') or 
                           'detail' in link.get('href', '') or
                           'view' in link.get('href', ''))]
        
        if detail_links:
            print(f"âœ… ìƒì„¸ í˜ì´ì§€ ë§í¬ {len(detail_links)}ê°œ ë°œê²¬:")
            for i, link in enumerate(detail_links[:5]):  # ì²˜ìŒ 5ê°œë§Œ
                href = link.get('href', '')
                text = link.get_text(strip=True)[:50]
                print(f"  {i+1}. {text} -> {href}")
        
        return detail_links
    
    async def _test_file_download_process(self, page):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
        print("\n5ï¸âƒ£ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸:")
        
        # ìƒì„¸ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
        detail_links = await page.query_selector_all('a[href*="Detail"], a[href*="detail"], a[href*="view"]')
        
        if not detail_links:
            print("âŒ ìƒì„¸ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # JavaScriptë¡œ ëª¨ë“  ë§í¬ ê²€ìƒ‰
            all_links = await page.evaluate("""
                () => {
                    const links = Array.from(document.querySelectorAll('a[href]'));
                    return links.map(link => ({
                        href: link.href,
                        text: link.textContent.trim(),
                        onclick: link.onclick ? link.onclick.toString() : null
                    })).filter(link => link.text.length > 0);
                }
            """)
            
            print(f"ğŸ“‹ í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ({len(all_links)}ê°œ):")
            for i, link in enumerate(all_links[:10]):  # ì²˜ìŒ 10ê°œë§Œ
                print(f"  {i+1}. {link['text'][:50]} -> {link['href']}")
                if link['onclick']:
                    print(f"      onclick: {link['onclick'][:100]}")
            
            return
        
        print(f"âœ… ìƒì„¸ í˜ì´ì§€ ë§í¬ {len(detail_links)}ê°œ ë°œê²¬")
        
        # ì²« ë²ˆì§¸ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼
        try:
            first_link = detail_links[0]
            link_text = await first_link.text_content()
            link_href = await first_link.get_attribute('href')
            
            print(f"ğŸ”— ì²« ë²ˆì§¸ ê³µê³  ì ‘ê·¼: {link_text}")
            print(f"ğŸ“„ ë§í¬: {link_href}")
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì¹´ìš´íŠ¸
            before_count = len(self.all_requests)
            
            await first_link.click()
            await page.wait_for_load_state('networkidle')
            
            # ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
            new_requests = self.all_requests[before_count:]
            print(f"ğŸ“Š ìƒˆë¡œìš´ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ {len(new_requests)}ê°œ:")
            for req in new_requests[-10:]:  # ë§ˆì§€ë§‰ 10ê°œë§Œ
                print(f"  - {req['method']} {req['url']}")
            
            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ì°¾ê¸°
            await self._analyze_attachments_in_detail(page)
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _analyze_attachments_in_detail(self, page):
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë¶„ì„"""
        print("\nğŸ“ ì²¨ë¶€íŒŒì¼ ë¶„ì„:")
        
        # ì²¨ë¶€íŒŒì¼ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
        attachment_selectors = [
            'a[onclick*="download"]',
            'a[href*="download"]',
            'a[onclick*="atchFile"]',
            'a[onclick*="file"]',
            '.attach a',
            '.file a',
            'a:has-text("ë‹¤ìš´ë¡œë“œ")',
            'a:has-text("ì²¨ë¶€")'
        ]
        
        found_attachments = []
        for selector in attachment_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    found_attachments.extend(elements)
                    print(f"âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
            except:
                pass
        
        if not found_attachments:
            print("âŒ ì²¨ë¶€íŒŒì¼ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒŒì¼ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
            page_text = await page.text_content('body')
            file_keywords = ['ì²¨ë¶€', 'ë‹¤ìš´ë¡œë“œ', 'íŒŒì¼', 'hwp', 'pdf', 'doc']
            for keyword in file_keywords:
                if keyword in page_text:
                    print(f"âœ… '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
            
            return
        
        # ì²¨ë¶€íŒŒì¼ ë§í¬ ìƒì„¸ ë¶„ì„
        print(f"ğŸ“ ì´ {len(found_attachments)}ê°œì˜ ì²¨ë¶€íŒŒì¼ ë§í¬ ë°œê²¬:")
        
        for i, attachment in enumerate(found_attachments[:5]):  # ì²˜ìŒ 5ê°œë§Œ
            try:
                text = await attachment.text_content()
                href = await attachment.get_attribute('href')
                onclick = await attachment.get_attribute('onclick')
                
                print(f"  {i+1}. {text.strip()}")
                print(f"     href: {href}")
                print(f"     onclick: {onclick}")
                
                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                if i == 0:  # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ ì‹œë„
                    await self._attempt_download(page, attachment)
                
            except Exception as e:
                print(f"     âŒ ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    async def _attempt_download(self, page, attachment):
        """ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹œë„"""
        print("\nğŸ“¥ ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì‹œë„:")
        
        try:
            # ë‹¤ìš´ë¡œë“œ ì´ë²¤íŠ¸ ëª¨ë‹ˆí„°ë§
            download_started = False
            
            async def handle_download(download):
                nonlocal download_started
                download_started = True
                print(f"âœ… ë‹¤ìš´ë¡œë“œ ì‹œì‘: {download.suggested_filename}")
                print(f"ğŸ“„ ë‹¤ìš´ë¡œë“œ URL: {download.url}")
                
                # ë‹¤ìš´ë¡œë“œ ì €ì¥ (ì‹¤ì œë¡œëŠ” ì·¨ì†Œ)
                try:
                    await download.cancel()
                    print("âœ… ë‹¤ìš´ë¡œë“œ ì·¨ì†Œ (í…ŒìŠ¤íŠ¸ ëª©ì )")
                except:
                    pass
            
            page.on('download', handle_download)
            
            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§
            before_count = len(self.all_requests)
            
            # ë§í¬ í´ë¦­
            await attachment.click()
            await asyncio.sleep(3)
            
            # ë‹¤ìš´ë¡œë“œ í›„ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ë¶„ì„
            after_count = len(self.all_requests)
            download_requests = self.all_requests[before_count:after_count]
            
            if download_requests:
                print(f"ğŸ“Š ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ {len(download_requests)}ê°œ:")
                for req in download_requests:
                    print(f"  - {req['method']} {req['url']}")
                    print(f"    Headers: {json.dumps({k: v for k, v in req['headers'].items() if k.lower() in ['referer', 'cookie', 'user-agent']}, indent=2)}")
                    if req['post_data']:
                        print(f"    POST Data: {req['post_data']}")
            
            if not download_started and not download_requests:
                print("âŒ ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    async def _comprehensive_results_analysis(self):
        """ì¢…í•© ê²°ê³¼ ë¶„ì„"""
        print("\n" + "="*80)
        print("ğŸ“Š IRIS ì‚¬ì´íŠ¸ ì¢…í•© ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        # 1. ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í†µê³„
        print(f"\n1ï¸âƒ£ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ í†µê³„:")
        print(f"  - ì´ ìš”ì²­ ìˆ˜: {len(self.all_requests)}")
        print(f"  - ì´ ì‘ë‹µ ìˆ˜: {len(self.all_responses)}")
        
        # ë„ë©”ì¸ë³„ ìš”ì²­ ë¶„ì„
        domains = {}
        for req in self.all_requests:
            domain = req['url'].split('/')[2] if '://' in req['url'] else 'unknown'
            domains[domain] = domains.get(domain, 0) + 1
        
        print(f"  - ë„ë©”ì¸ë³„ ìš”ì²­:")
        for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True):
            print(f"    {domain}: {count}ê°œ")
        
        # 2. IRIS ê´€ë ¨ ìš”ì²­ ë¶„ì„
        iris_requests = [req for req in self.all_requests if 'iris.go.kr' in req['url']]
        print(f"\n2ï¸âƒ£ IRIS ê´€ë ¨ ìš”ì²­ ({len(iris_requests)}ê°œ):")
        
        for req in iris_requests:
            print(f"  - {req['method']} {req['url']}")
            if req['post_data']:
                print(f"    POST Data: {req['post_data']}")
        
        # 3. ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­
        download_requests = [req for req in self.all_requests 
                           if any(keyword in req['url'].lower() 
                                 for keyword in ['download', 'atchfile', 'file'])]
        
        if download_requests:
            print(f"\n3ï¸âƒ£ ë‹¤ìš´ë¡œë“œ ê´€ë ¨ ìš”ì²­ ({len(download_requests)}ê°œ):")
            for req in download_requests:
                print(f"  - {req['method']} {req['url']}")
                if req['headers'].get('referer'):
                    print(f"    Referer: {req['headers']['referer']}")
        
        # 4. ì¿ í‚¤ ì •ë³´ (ë§ˆì§€ë§‰ ìš”ì²­ì—ì„œ)
        if iris_requests:
            last_request = iris_requests[-1]
            cookie_header = last_request['headers'].get('cookie', '')
            if cookie_header:
                print(f"\n4ï¸âƒ£ ì¿ í‚¤ ì •ë³´:")
                cookies = cookie_header.split('; ')
                for cookie in cookies:
                    if '=' in cookie:
                        name, value = cookie.split('=', 1)
                        if name in ['JSESSIONID', 'WMONID', 'SESSION']:
                            print(f"  - {name}: {value}")
        
        # 5. êµ¬í˜„ ê¶Œì¥ì‚¬í•­
        print(f"\n5ï¸âƒ£ ìŠ¤í¬ë˜í¼ êµ¬í˜„ ê¶Œì¥ì‚¬í•­:")
        print("  1. ì„¸ì…˜ ê´€ë¦¬: JSESSIONID ì¿ í‚¤ ë³´ì¡´ í•„ìˆ˜")
        print("  2. User-Agent ì„¤ì •: ë¸Œë¼ìš°ì € í™˜ê²½ ëª¨ë°©")
        print("  3. Referer í—¤ë”: ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œ í•„ìˆ˜")
        print("  4. í˜ì´ì§€ ë¡œë”©: ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° í•„ìš”")
        print("  5. ì—ëŸ¬ ì²˜ë¦¬: ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„")
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        analysis_result = {
            'timestamp': time.time(),
            'total_requests': len(self.all_requests),
            'iris_requests': len(iris_requests),
            'download_requests': len(download_requests),
            'domains': domains,
            'sample_requests': iris_requests[:10],  # ìƒ˜í”Œ ìš”ì²­
            'recommendations': [
                "ì„¸ì…˜ ê´€ë¦¬: JSESSIONID ì¿ í‚¤ ë³´ì¡´ í•„ìˆ˜",
                "User-Agent ì„¤ì •: ë¸Œë¼ìš°ì € í™˜ê²½ ëª¨ë°©",
                "Referer í—¤ë”: ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œ í•„ìˆ˜",
                "í˜ì´ì§€ ë¡œë”©: ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° í•„ìš”",
                "ì—ëŸ¬ ì²˜ë¦¬: ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„"
            ]
        }
        
        with open('/tmp/iris_comprehensive_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ /tmp/iris_comprehensive_analysis.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def main():
    analyzer = IrisComprehensiveAnalyzer()
    await analyzer.comprehensive_analysis()


if __name__ == "__main__":
    asyncio.run(main())